{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "md5pi22bNkk3"
   },
   "source": [
    "MODULE 3 | LESSON 4\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "# **Singular Value Decomposition of Matrices**\n",
    "|  |  |\n",
    "|:---|:---|\n",
    "|**Reading Time** 60 minutes |   |\n",
    "|**Prior Knowledge** Basic Matrix Operation, Linear Algebra, Basic Python |   |\n",
    "|**Keywords** Singular Value Decomposition (SVD), Full and economy SVD, matrix approximation  |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wYUA3ohnN8Ta"
   },
   "source": [
    "*In the previous lesson, we introduced methods to decompose a symmetric matrix. However, manytimes, the dataset or a matrix we have is not a symmetric matrix. In this lesson, we will introduce a general method to decompose a matrix. This method is called Singular Value Decomposition or SVD. We will go through the definition and properties of SVD and then present an application.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19oB1PsWGAsT"
   },
   "source": [
    "## **1. Introducing Singular Value Decomposition (SVD)**\n",
    "**Singular value decomposition (SVD)** is a very popular method in numerical linear algebra. It is commonly used to solve for a linear regression problem when the matrix is not squared. It can also be used as a data dimension reduction method. One of the examples is to use SVD as the basis for principal component analysis. SVD is also a foundational numeric method for machine learning algorithms.\n",
    "<br>\n",
    "In the last module, we discussed how to diagonalize a symmetric matrix. If $A$ is a 3 by 3 symmetric matrix, we can diagonalize or factor $A$ as follows:\n",
    "<br>\n",
    "<br>\n",
    "$$A=\\begin{bmatrix}\n",
    "| & | &  |\\\\\n",
    "v_{1} & v_{2} & v_{3} \\\\\n",
    "| & | & |\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "\\lambda_{1} & 0 & 0 \\\\\n",
    "0 & \\lambda_{2} & 0 \\\\\n",
    "0 & 0 & \\lambda_{3}\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "| & | &  |\\\\\n",
    "v_{1} & v_{2} & v_{3} \\\\\n",
    "| & | & |\n",
    "\\end{bmatrix}^{T}$$\n",
    "<br>\n",
    "where\n",
    "<br>\n",
    "$v_{1} , v_{2} , v_{3}$ are eigenvectors of $A$\n",
    "<br>\n",
    "$\\lambda_{1} , \\lambda_{2} , \\lambda_{3}$ are eigenvalues of $A$\n",
    "<br>\n",
    "<br>\n",
    "However, most of the time, the datasets or a matrix we have will not have symmetry. Hence, we need a method to diagonalize a general matrix. This method is singular value decomposition.\n",
    "<br>\n",
    "<br>\n",
    "## **2. Definition of Singular Value Decomposition**\n",
    "### **2.1. Matrix Representation of SVD**\n",
    "**Singular value decomposition** is a method to decompose a matrix into three matrices. For any $m\\times n$ matrix $A$ with real entries, there exists a factorization of the form:\n",
    "<br>\n",
    "<br>\n",
    "$$A=U\\Sigma V^{T}$$\n",
    "<br>\n",
    "Where:\n",
    "<br>\n",
    "$U$ is an $m\\times m$ unitary and orthogonal, hence orthonormal matrix\n",
    "<br>\n",
    "$\\Sigma$ is an $m\\times n$ rectangular diagonal matrix with non-zero real numbers on the diagonal\n",
    "<br>\n",
    "$V^{T}$ is the transpose of $V$; $V$ is an $n\\times n$ unitary and orthogonal (orthonormal) matrix\n",
    "<br>\n",
    "<br>\n",
    "We can also rewrite the above formula in the following matrix form assuming $m\\ge n$.\n",
    "<br>\n",
    "<br>\n",
    "$$\\begin{bmatrix}\n",
    " &  &  &  \\\\\n",
    "| & | &  & |\\\\\n",
    "a_{1} & a_{2} & ... & a_{n} \\\\\n",
    "| & | &  & |\\\\\n",
    " &  &  &\n",
    "\\end{bmatrix}=\\begin{bmatrix}\n",
    " &  &  &  \\\\\n",
    "| & | &  & |\\\\\n",
    "u_{1} & u_{2} & ... & u_{m} \\\\\n",
    "| & | &  & |\\\\\n",
    " &  &  &\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "\\sigma_{1} & 0 & ... & 0 \\\\\n",
    "0 & \\sigma_{2} & ... & 0 \\\\\n",
    "0 & 0 & ... & 0 \\\\\n",
    "0 & 0 & ... & \\sigma_{n} \\\\\n",
    "0 & 0 & ... & 0\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    " &  &  &  \\\\\n",
    "| & | & | & | \\\\\n",
    "v_{1} & v_{2} & ... & v_{n} \\\\\n",
    "| & | & | & | \\\\\n",
    " &  &  &\n",
    "\\end{bmatrix}^{T}$$\n",
    "<br>\n",
    "where $a$ are $n$ column vectors in matrix $A$, $u$ are $m$ column vectors in matrix $U$, and $v$ are $n$ column vectors in matrix $V$.\n",
    "<br>\n",
    "The above matrix dimensions are as follows:\n",
    "$$\\left[ m\\times n \\right]=\\left[ m\\times m \\right]\\left[ m\\times n \\right]\\left[ n\\times n \\right]$$\n",
    "<br>\n",
    "<br>\n",
    "The diagonal entries $\\sigma_{i}$ of $\\Sigma$ are known as the **singular values** of $A$. They are typically arranged in descending order ($\\sigma_{1}$ ≥ $\\sigma_{2}$ ≥ ... ≥ $\\sigma_{min(m,n)}$ ≥ 0). Since the above $\\Sigma$ matrix assumes $m \\ge n$, there can be at most n positive singular values. The values of the rest of the rows in $\\Sigma$ are 0.\n",
    "<br>\n",
    "The vectors in matrix $U$ are called the **left singular vectors** of $A$.\n",
    "<br>\n",
    "The vectors in matrix $V$ are called the **right singular vectors** of $A$.\n",
    "<br>\n",
    "<br>\n",
    "### **2.2. Geometric Interpretation of Singular Value Decomposition (SVD)**\n",
    "SVD can be interpreted geometrically as a composition of three transformations:\n",
    "1. A rotation or reflection $U$\n",
    "2. A scaling or dilating along coordinate axes $\\Sigma$\n",
    "3. Another rotation or reflection $V^{T}$\n",
    "\n",
    "This interpretation helps in understanding how SVD captures the essential structure of linear transformations. The following graph illustrates the decomposition.\n",
    "<br>\n",
    "<br>\n",
    "**Figure 1: Visualization for Singular Value Decomposition**\n",
    "\n",
    "![Diagram illustrating the Singular Value Decomposition (SVD) process, showing transformations: a linear transformation (A), rotations (U and V^T), and dilation (Σ), represented on a 2D plane with vectors.](images/FD_M3_L4_decomposition.jpg)\n",
    "<br>\n",
    "<br>\n",
    "## **3. Properties of Singular Value Decomposition (SVD)**\n",
    "In this section, we will discuss some of the key properties for SVD.\n",
    "<br>\n",
    "<br>\n",
    "##### **Property 1.**\n",
    "Since the vectors in $U$ and $V$ are unitary and orthogonal,\n",
    "<br>\n",
    "$U^{T}U=UU^{T}=I_{m\\times m}$   (identity matrix)\n",
    "<br>\n",
    "$V^{T}V=VV^{T}=I_{n\\times n}$   (identity matrix)\n",
    "<br>\n",
    "<br>\n",
    "##### **Property 2.**\n",
    "The number of non-zero singular values in matrix $\\Sigma$ is the rank of the matrix $A$.\n",
    "<br>\n",
    "<br>\n",
    "##### **Property 3.**\n",
    "Since singular values $\\sigma$s are arranged in descending order in the $\\Sigma$ matrix, the first column of $U$(which is $u_{1}$) and the first column of $V$(which is $v_{1}$) corresponding to $\\sigma_{1}$ are more important in describing information in matrix $A$ than the second columns of the $U$ and $V$ matrices. The first columns of $U$ and $V$ are also more important than the third columns in describing matrix $A$, and so on and so forth. Because of this property, when some $\\sigma$'s values are very small, it means they contain only a very small amount of the information of matrix $A$. We can drop these $\\sigma$s and their corresponding columns in the $U$ and $V$ matrices.\n",
    "<br>\n",
    "<br>\n",
    "##### **Property 4.**\n",
    "In the last section, we mentioned that the number of non-zero singular values in the $\\Sigma$ matrix is the minimum of $m$ and $n$. Hence, there can be rows of all $0$s at the lower part of matrix $\\Sigma$. As a result, when we conduct a dot product of matrix $U$ with matrix $\\Sigma$, the corresponding vectors of $U$ will multiply with all the rows of $0$s in matrix $\\Sigma$ and give $0$ result. With this result, we can simply SVD to a reduced form by dropping rows of all $0$ in matrix $\\Sigma$ and the corresponding vectors in matrix $U$. The following Figure 2 demonstrates this property.\n",
    "<br>\n",
    "<br>\n",
    "**Figure 2: Full SVD vs. Economy/Reduced SVD**\n",
    "\n",
    "![Diagram comparing Full and Reduced Singular Value Decomposition (SVD), showing matrices A, U, Σ, and V^T with their respective components and dimensions.](images/FD_M3_L4_reduced_SVD.jpg)\n",
    "<br>\n",
    "<br>\n",
    "In the first equation in Figure 2, we can see when we conduct a dot product for $U$ and $\\Sigma$, $\\hat{U}^{\\bot }$ part of $U$ will multiply with $0$ of $\\Sigma$. The result  will be $0$. Hence, we can drop them without losing information in matrix A. The second equation is the reduced form of SVD. It is called **reduced SVD** or **economy SVD**.\n",
    "<br>\n",
    "<br>\n",
    "## **4. Matrix Approximation Using SVD**\n",
    "In this section, we are going to demonstrate how to use SVD to approximate a matrix. This is the most popular reason to use SVD.\n",
    "<br>\n",
    "Assume for the $m \\times n$ matrix A, the rank  is $r$, and $r\\lt min(m,n)$. We can write matrix $A$'s SVD as follows.\n",
    "<br>\n",
    "<br>\n",
    "$$A=\\begin{bmatrix}\n",
    " &  &  &  \\\\\n",
    "| & | &  & |\\\\\n",
    "a_{1} & a_{2} & ... & a_{n} \\\\\n",
    "| & | &  & |\\\\\n",
    " &  &  & \\\\\n",
    " &  &  &\n",
    "\\end{bmatrix}=\\begin{bmatrix}\n",
    " &  &  &  \\\\\n",
    "| & | &  & |\\\\\n",
    "u_{1} & u_{2} & ... & u_{m} \\\\\n",
    "| & | &  & |\\\\\n",
    " &  &  & \\\\\n",
    " &  &  &\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "\\sigma_{1} & 0 & ... & 0 &0 \\\\\n",
    "0 & \\sigma_{2} & ... & 0 &0\\\\\n",
    "0 & 0 & ... & 0 &0\\\\\n",
    "0 & 0 & ... & \\sigma_{r} &0\\\\\n",
    "0 & 0 & ... & 0 &0\\\\\n",
    "0 & 0 & ... & 0 &0\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    " &  &  &  \\\\\n",
    "| & | & | & | \\\\\n",
    "v_{1} & v_{2} & ... & v_{n} \\\\\n",
    "| & | & | & | \\\\\n",
    " &  &  & \\\\\n",
    " &  &  &\n",
    "\\end{bmatrix}^{T}$$\n",
    "<br>\n",
    "<br>\n",
    "From the singular values in $\\Sigma$, we know $\\sigma_{1}\\ge \\sigma_{2}\\ge ...\\ge \\sigma_{r}\\gt \\sigma_{r+1}=...\\sigma_{min(m,n)}=0$. We can expand matrix A as follows.\n",
    "<br>\n",
    "<br>\n",
    "$$A=\\sum_{k=1}^{r}\\sigma_{k}u_{k}v_{k}^{T}=\\sigma_{1}u_{1}v_{1}^{T}+\\sigma_{2}u_{2}v_{2}^{T}+...+\\sigma_{r-1}u_{r-1}v_{r-1}^{T}+\\sigma_{r}u_{r}v_{r}^{T}$$\n",
    "<br>\n",
    "<br>\n",
    "This is called the Eckart-Young theorem. According to the theorem, we can represent matrix $A$ by only using leading r vectors in matrix $U$ and leading r vectors in matrix $V$ because in the $\\Sigma$ matrix, there are only $r$ non-zero $\\sigma$s.\n",
    "<br>\n",
    "We also know that $\\sigma$s in $\\Sigma$ are in descending order. If the $\\sigma$s on the right side of the equation are too small, we can drop them from the equation. Then, we can approximate matrix $A$ by using only $\\sigma$s with large values. Say we only want to keep $\\hat{r}$ leading $\\sigma$s in the equation, where $r \\ge \\hat{r}$. The approximated matrix $A$ can be expressed as follows.\n",
    "<br>\n",
    "<br>\n",
    "$$\\tilde{A}=\\sum_{k=1}^{\\hat{r}}\\sigma_{k}u_{k}v_{k}^{T}=\\sigma_{1}u_{1}v_{1}^{T}+\\sigma_{2}u_{2}v_{2}^{T}+...+\\sigma_{\\hat{r}-1}u_{\\hat{r}-1}v_{\\hat{r}-1}^{T}+\\sigma_{\\hat{r}}u_{\\hat{r}}v_{\\hat{r}}^{T}$$\n",
    "<br>\n",
    "<br>\n",
    "$\\tilde{A}$ is the approximation of A. Based on the above equation and explanation, $\\tilde{A}$ contains fewer data points than $A$ but keeps the majority of the information from the original matrix $A$. We reduce data dimension using SVD to approximate a matrix. This is due to the property of SVD that allows us to solve least-squared issues or run machine learning algorithms more efficiently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_afokN2FANh6"
   },
   "source": [
    "## **5. Application of Singular Value Decomposition (SVD)**\n",
    "In this section, we are going to use some financial data to demonstrate the usage of SVD. We will use the stock price data for Apple, Ford Motor, Pfizer, and Walmart to show how to apply SVD. First, let's download some Python libraries for this application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ylQm_Vry1J0Z"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yfin\n",
    "import seaborn as sns\n",
    "import math\n",
    "from numpy import linalg as LA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v2tyDDe9JeFy"
   },
   "source": [
    "### **5.1 Importing Data**\n",
    "\n",
    "Next, we are going to download stock price data from Yahoo Finance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "executionInfo": {
     "elapsed": 2020,
     "status": "ok",
     "timestamp": 1729705756976,
     "user": {
      "displayName": "Greg Ciresi",
      "userId": "14255040454306907358"
     },
     "user_tz": 240
    },
    "id": "3h4N-2J5ISam",
    "outputId": "26bbf3f1-e6f0-477e-bcea-f94aeec62c35"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  4 of 4 completed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Ticker</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>F</th>\n",
       "      <th>PFE</th>\n",
       "      <th>WMT</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-02</th>\n",
       "      <td>40.479843</td>\n",
       "      <td>8.594532</td>\n",
       "      <td>25.560186</td>\n",
       "      <td>29.111248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-03</th>\n",
       "      <td>40.472797</td>\n",
       "      <td>8.662418</td>\n",
       "      <td>25.749571</td>\n",
       "      <td>29.365179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-04</th>\n",
       "      <td>40.660770</td>\n",
       "      <td>8.811770</td>\n",
       "      <td>25.805691</td>\n",
       "      <td>29.391754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-05</th>\n",
       "      <td>41.123711</td>\n",
       "      <td>8.961122</td>\n",
       "      <td>25.854788</td>\n",
       "      <td>29.565964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-08</th>\n",
       "      <td>40.970966</td>\n",
       "      <td>8.927178</td>\n",
       "      <td>25.567196</td>\n",
       "      <td>30.002975</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Ticker           AAPL         F        PFE        WMT\n",
       "Date                                                 \n",
       "2018-01-02  40.479843  8.594532  25.560186  29.111248\n",
       "2018-01-03  40.472797  8.662418  25.749571  29.365179\n",
       "2018-01-04  40.660770  8.811770  25.805691  29.391754\n",
       "2018-01-05  41.123711  8.961122  25.854788  29.565964\n",
       "2018-01-08  40.970966  8.927178  25.567196  30.002975"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download stock prices from Yahoo Finance and set the time period for download\n",
    "start = datetime.date(2018, 1, 2)\n",
    "end = datetime.date(2023, 12, 31)\n",
    "stocks = yfin.download([\"AAPL\", \"F\", \"WMT\",\"PFE\"], start, end, auto_adjust = False)[\"Adj Close\"]\n",
    "stocks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Pd_cxTAgIgE6"
   },
   "outputs": [],
   "source": [
    "stocks.index = pd.to_datetime(stocks.index).strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HuIhKuC6JqYw"
   },
   "source": [
    "### **5.2 Calculating Returns**\n",
    "Now, let's calculate the returns of these stock prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "executionInfo": {
     "elapsed": 443,
     "status": "ok",
     "timestamp": 1729705760729,
     "user": {
      "displayName": "Greg Ciresi",
      "userId": "14255040454306907358"
     },
     "user_tz": 240
    },
    "id": "4OpCTKOkInHz",
    "outputId": "5d7eeb46-85d8-4922-e96c-59f57c1de0c3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Ticker</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>F</th>\n",
       "      <th>WMT</th>\n",
       "      <th>PFE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-03</th>\n",
       "      <td>-0.000174</td>\n",
       "      <td>0.007899</td>\n",
       "      <td>0.008723</td>\n",
       "      <td>0.007409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-04</th>\n",
       "      <td>0.004644</td>\n",
       "      <td>0.017241</td>\n",
       "      <td>0.000905</td>\n",
       "      <td>0.002179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-05</th>\n",
       "      <td>0.011385</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.005927</td>\n",
       "      <td>0.001903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-08</th>\n",
       "      <td>-0.003714</td>\n",
       "      <td>-0.003788</td>\n",
       "      <td>0.014781</td>\n",
       "      <td>-0.011123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-09</th>\n",
       "      <td>-0.000115</td>\n",
       "      <td>-0.005323</td>\n",
       "      <td>-0.012007</td>\n",
       "      <td>-0.001097</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Ticker          AAPL         F       WMT       PFE\n",
       "Date                                              \n",
       "2018-01-03 -0.000174  0.007899  0.008723  0.007409\n",
       "2018-01-04  0.004644  0.017241  0.000905  0.002179\n",
       "2018-01-05  0.011385  0.016949  0.005927  0.001903\n",
       "2018-01-08 -0.003714 -0.003788  0.014781 -0.011123\n",
       "2018-01-09 -0.000115 -0.005323 -0.012007 -0.001097"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In order to calculate the returns of the stocks, we need to drop the NA rows.\n",
    "stocks_returns = stocks[[\"AAPL\", \"F\", \"WMT\",\"PFE\"]].dropna().pct_change()\n",
    "stocks_returns = stocks_returns.dropna()\n",
    "stocks_returns.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uUgsVadOJw7U"
   },
   "source": [
    "### **5.3 Computing the SVD**\n",
    "\n",
    "We now have the stock price return dataset for four stocks. We can simply calculate SVD using the following Python function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "zBWw7gnjJBF8"
   },
   "outputs": [],
   "source": [
    "# Perform SVD for stock returns\n",
    "U, s, VT = np.linalg.svd(stocks_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 459,
     "status": "ok",
     "timestamp": 1729705766372,
     "user": {
      "displayName": "Greg Ciresi",
      "userId": "14255040454306907358"
     },
     "user_tz": 240
    },
    "id": "zir7lAXNJWs3",
    "outputId": "5d595d71-1238-4b1e-def1-4fac60e9f798"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock Returns Matrix Dimension:\n",
      "(1508, 4)\n",
      "\n",
      "Dimension of Matrix U:\n",
      "(1508, 1508)\n",
      "\n",
      "Singular values:\n",
      "[1.11696764 0.72489523 0.55820593 0.47159242]\n",
      "\n",
      "Dimension of Matrix V^T:\n",
      "(4, 4)\n"
     ]
    }
   ],
   "source": [
    "# Present the result\n",
    "print(\"Stock Returns Matrix Dimension:\")\n",
    "print(stocks_returns.shape)\n",
    "print(\"\\nDimension of Matrix U:\")\n",
    "print(U.shape)\n",
    "print(\"\\nSingular values:\")\n",
    "print(s)\n",
    "print(\"\\nDimension of Matrix V^T:\")\n",
    "print(VT.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iQwv-mePKTwy"
   },
   "source": [
    "# **6. Comparing PCA and SVD**\n",
    "\n",
    "So far, it has been very straightforward to calculate the SVD of a matrix using Python. Let's make things a little bit more interesting. We are going to show the connection between SVD and principal component analysis (PCA) (Brunton et al.).\n",
    "<br>\n",
    "In Module 1, we briefly discussed how to obtain principal components using eigenvectors and eigenvalues. In this section, we will show you how to use the SVD of a matrix to get eigenvalues and eigenvectors.\n",
    "<br>\n",
    "Remember in Module 2, before we get to principal components, we need to standardize the data. Let's do that first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "executionInfo": {
     "elapsed": 179,
     "status": "ok",
     "timestamp": 1729705771603,
     "user": {
      "displayName": "Greg Ciresi",
      "userId": "14255040454306907358"
     },
     "user_tz": 240
    },
    "id": "K9A2iQt8vJft",
    "outputId": "d9e9ea77-8d89-4c17-8a89-da975118e8ed"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Ticker</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>F</th>\n",
       "      <th>WMT</th>\n",
       "      <th>PFE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-03</th>\n",
       "      <td>-0.070352</td>\n",
       "      <td>0.286266</td>\n",
       "      <td>0.584128</td>\n",
       "      <td>0.446783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-04</th>\n",
       "      <td>0.171120</td>\n",
       "      <td>0.647637</td>\n",
       "      <td>0.030003</td>\n",
       "      <td>0.124458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-05</th>\n",
       "      <td>0.508937</td>\n",
       "      <td>0.636329</td>\n",
       "      <td>0.385974</td>\n",
       "      <td>0.107394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-08</th>\n",
       "      <td>-0.247765</td>\n",
       "      <td>-0.165771</td>\n",
       "      <td>1.013527</td>\n",
       "      <td>-0.695407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-09</th>\n",
       "      <td>-0.067368</td>\n",
       "      <td>-0.225153</td>\n",
       "      <td>-0.885172</td>\n",
       "      <td>-0.077478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Ticker          AAPL         F       WMT       PFE\n",
       "Date                                              \n",
       "2018-01-03 -0.070352  0.286266  0.584128  0.446783\n",
       "2018-01-04  0.171120  0.647637  0.030003  0.124458\n",
       "2018-01-05  0.508937  0.636329  0.385974  0.107394\n",
       "2018-01-08 -0.247765 -0.165771  1.013527 -0.695407\n",
       "2018-01-09 -0.067368 -0.225153 -0.885172 -0.077478"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standardize stock returns dataset\n",
    "stocks_returns_means = stocks_returns.mean()\n",
    "stocks_returns_stds = stocks_returns.std()\n",
    "standardized_returns = (stocks_returns - stocks_returns_means) / stocks_returns_stds\n",
    "standardized_returns.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EtaX6h7xNHQV"
   },
   "source": [
    "Next, we want to calculate the covariance of the standardized returns matrix. The matrix form of the covariance equation is as follows.\n",
    "<br>\n",
    "<br>\n",
    "$$\\left( \\frac{standardized \\;\\;returns\\;\\;matrix}{\\sqrt{n-1}} \\right)^{T}\\bullet \\left( \\frac{standardized  \\;\\;returns\\;\\;matrix}{\\sqrt{n-1}} \\right)$$\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 221,
     "status": "ok",
     "timestamp": 1729705775070,
     "user": {
      "displayName": "Greg Ciresi",
      "userId": "14255040454306907358"
     },
     "user_tz": 240
    },
    "id": "Bfsnx89Jv4YB",
    "outputId": "a64a2a1b-50cf-48cf-d461-78a93fbc5a6a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Ticker</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>F</th>\n",
       "      <th>WMT</th>\n",
       "      <th>PFE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ticker</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.376460</td>\n",
       "      <td>0.365246</td>\n",
       "      <td>0.324438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F</th>\n",
       "      <td>0.376460</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.185248</td>\n",
       "      <td>0.224384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WMT</th>\n",
       "      <td>0.365246</td>\n",
       "      <td>0.185248</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.296375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PFE</th>\n",
       "      <td>0.324438</td>\n",
       "      <td>0.224384</td>\n",
       "      <td>0.296375</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Ticker      AAPL         F       WMT       PFE\n",
       "Ticker                                        \n",
       "AAPL    1.000000  0.376460  0.365246  0.324438\n",
       "F       0.376460  1.000000  0.185248  0.224384\n",
       "WMT     0.365246  0.185248  1.000000  0.296375\n",
       "PFE     0.324438  0.224384  0.296375  1.000000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate covariance for standardized return matrix\n",
    "standardized_returns_dvd_sqrt_n=(standardized_returns/math.sqrt(len(standardized_returns)-1))\n",
    "standardized_returns_cov = standardized_returns_dvd_sqrt_n.T@standardized_returns_dvd_sqrt_n\n",
    "standardized_returns_cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VGWefCb837ZO"
   },
   "source": [
    "Now, let's do some simple matrix manipulation of the covariance of standardized returns. Let's assume matrix $B$ is the $\\left( \\frac{standardized \\;\\;returns\\;\\;matrix}{\\sqrt{n-1}} \\right)$. Hence, we can rewrite the covariance of standardized returns as follows.\n",
    "<br>\n",
    "<br>\n",
    "$$Covariance = B^{T}B$$\n",
    "<br>\n",
    "<br>\n",
    "With SVD, we can write matrix $B$ as $B=U\\Sigma V^{T}$. Then, its transpose is $B^{T}=V\\Sigma U^{T}$. Then, we can calculate the covariance of the standardized returns as follows.\n",
    "<br>\n",
    "<br>\n",
    "$$Covariance =B^{T}B=V\\Sigma U^{T}U\\Sigma V^{T}=V\\Sigma\\Sigma V^{T}=V\\Sigma^{2} V^{T}$$\n",
    "<br>\n",
    "<br>\n",
    "Because the property of $U^{T}U=I$ is an identity matrix, we can drop $U^{T}U$. Next, let's conduct the following calculation.\n",
    "<br>\n",
    "<br>\n",
    "$$B^{T}BV=V\\Sigma^{2} V^{T}V=V\\Sigma^{2}$$\n",
    "<br>\n",
    "<br>\n",
    "We can drop $V^{T}V$ because $V^{T}V=I$ is an identity matrix.\n",
    "Does the equation above remind you of something? Yes, the equation shows that the column vectors in matrix $V$ are the eigenvectors of $B^{T}B$. The squared singular values in matrix $\\Sigma$ are eigenvalues of $B^{T}B$. Hence, $Bv_{1}$is the first principal component and the corresponding $\\sigma_{1}^{2}$ is the variance of the dataset $B$ explained by the first principal component.\n",
    "<br>\n",
    "Now, you actually have two ways to calculate principal components. The first method, as described in Module 1, Lesson 4, is by calculating the eigenvalues and eigenvectors of the covariance matrix of the data. The second method is to calculate the SVD of the data. Then, matrix $V$ will contain eigenvectors, and squared singular values will be eigenvalues.\n",
    "<br>\n",
    "Now let's try both methods to get eigenvalues and eigenvectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 177,
     "status": "ok",
     "timestamp": 1729705781573,
     "user": {
      "displayName": "Greg Ciresi",
      "userId": "14255040454306907358"
     },
     "user_tz": 240
    },
    "id": "bFl5eN1j5lES",
    "outputId": "3e2f5ab0-f487-452a-a243-0ac627888d76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Squared Singular values (eigenvalues):\n",
      "[1.89474893 0.83555324 0.71064962 0.55904821]\n",
      "\n",
      "Matrix V (eigenvectors)\n",
      "[[ 0.5663476   0.1401175  -0.21637521 -0.78281495]\n",
      " [ 0.45964808  0.75759195  0.01833654  0.46307867]\n",
      " [ 0.48586762 -0.53226808 -0.55857182  0.4106347 ]\n",
      " [ 0.48156691 -0.3508735   0.80052674  0.0643276 ]]\n"
     ]
    }
   ],
   "source": [
    "# Use SVD to calculate eigenvectors and eigenvalues of the covariance matrix of standardized returns\n",
    "U_st_return, s_st_return, VT_st_return = np.linalg.svd(standardized_returns_dvd_sqrt_n)\n",
    "print(\"\\nSquared Singular values (eigenvalues):\")\n",
    "print(s_st_return**2)\n",
    "print(\"\\nMatrix V (eigenvectors)\")\n",
    "print(VT_st_return.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 291,
     "status": "ok",
     "timestamp": 1729705784683,
     "user": {
      "displayName": "Greg Ciresi",
      "userId": "14255040454306907358"
     },
     "user_tz": 240
    },
    "id": "OfWxq--a8NiM",
    "outputId": "52903aa7-ddd0-4c86-b761-343768253f66"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.89474893, 0.83555324, 0.71064962, 0.55904821])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the method from Module 1 Lesson 4 to calculate eigenvectors and eigenvalues of the covariance matrix of standardized returns\n",
    "eigenvalues, eigenvectors = LA.eig(standardized_returns_cov)\n",
    "idx = np.argsort(eigenvalues)[::-1]\n",
    "eigenvalues = eigenvalues[idx]\n",
    "eigenvectors = eigenvectors[:, idx]\n",
    "eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 416,
     "status": "ok",
     "timestamp": 1729705789719,
     "user": {
      "displayName": "Greg Ciresi",
      "userId": "14255040454306907358"
     },
     "user_tz": 240
    },
    "id": "gDxW4N788s1g",
    "outputId": "26153fb2-f08c-45e7-9c40-37bee067b0d4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.5663476 ,  0.1401175 , -0.21637521, -0.78281495],\n",
       "       [-0.45964808,  0.75759195,  0.01833654,  0.46307867],\n",
       "       [-0.48586762, -0.53226808, -0.55857182,  0.4106347 ],\n",
       "       [-0.48156691, -0.3508735 ,  0.80052674,  0.0643276 ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Gnu-tRPZQ4T"
   },
   "source": [
    "We can see from the above result that both methods generate the same eigenvalues. Both methods also generate the same eigenvectors except the first one. Looking closely, we can see they only differ in regards to the negative sign. For eigenvectors, they are actually the same because the negative sign does not change the direction nor the scale of both eigenvectors.\n",
    "<br>\n",
    "You can use either method to obtain the eigenvectors and eigenvalues of a covariance matrix. There is no agreement on which one is better. We usually use SVD for implementing PCA, image compression, and recommendation systems. We would use the method in Module 1, Lesson 4 for solving least-square problems.\n",
    "<br>\n",
    "One key difference between PCA and SVD is how they handle starting points. In general, PCA works on the covariance or correlation matrix. PCA also standardizes the data matrix. However, SVD can simply work straight on the data matrix. If SVD does not standardize the data matrix, the resulting eigenvalues can differ from the eigenvalues calculated from PCA.\n",
    "<br>\n",
    "<br>\n",
    "# **7. Conclusion**\n",
    "In this lesson, we first introduced SVD. We gave its definition, demonstrated its geometric interpretation, and discussed its properties. We then showed its application in Python. We showed how to use SVD to calculate eigenvalues and eigenvectors of a covariance matrix. We also pointed out the connections between SVD and PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gNbH8drBg0Am"
   },
   "source": [
    "**References**\n",
    "- Brunton, Steven L., et al. \"Chapter 1: Singular Value Decomposition (SVD) and Principal Components Analysis (PCA).\" University of Washington, 2015, https://faculty.washington.edu/sbrunton/me565/pdf/CHAPTER1.pdf."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
