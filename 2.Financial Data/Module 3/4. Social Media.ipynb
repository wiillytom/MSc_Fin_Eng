{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A_djuWRzMXkq"
   },
   "source": [
    "MODULE 4 | LESSON 4\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQfsxvu2MrAc"
   },
   "source": [
    "# **Social Media**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aSW9-996Miww"
   },
   "source": [
    "|  |  |\n",
    "|:---|:---|\n",
    "|**Reading Time** | 6h |\n",
    "|**Prior Knowledge** | API, function of social media in general  |\n",
    "|**Keywords** | API, youtube, reddit, subreddit, sentiment analysis, nltk, keywords |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t6JWZbCYFhmI"
   },
   "source": [
    "*Social media has become an integral part of modern life, transforming how we communicate, share information, and interact with the world around us. To understand the significance of social media data and its applications today, it's essential to examine the historical evolution of these platforms.*\n",
    "\n",
    "*The roots of social media can be traced back to the early days of the internet. In 1997, SixDegrees.com launched as the first recognizable social network site, allowing users to create profiles and list friends. This paved the way for subsequent platforms like Friendster, MySpace, and eventually Facebook, which revolutionized the social media landscape. As these platforms grew in popularity and functionality, they began generating vast amounts of user data. This data, encompassing everything from user profiles and connections to content interactions and behavioral patterns, quickly became a valuable resource for researchers, marketers, and businesses.*\n",
    "\n",
    "*The rise of social media data as a field of study and application coincided with the increasing sophistication of these platforms. Facebook's introduction of the News Feed in 2006, for instance, not only changed how users interacted with the platform but also provided a rich source of data on user preferences and behaviors. Today, social media data is used across various domains, including marketing, public health, political science, and social research. Platforms like YouTube and Reddit offer unique insights into user behavior, content consumption, and public discourse. Through APIs (application programming interfaces) provided by these platforms, researchers and developers can access and analyze this data in unprecedented ways.*\n",
    "\n",
    "*In this lesson, we will focus on the practical aspects of retrieving and working with social media data. We'll explore how to use APIs to collect data from YouTube and Reddit, demonstrating the real-world applications of social media data analysis. By understanding both the historical context and current methodologies, we can better appreciate the power and potential of social media data in today's digital landscape.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4tsZsezGth8R"
   },
   "source": [
    "## **1. What is Social Media?**\n",
    "\n",
    "We will provide some definitions as to what social media is even though each and every one of us probably has enough exposure to such platforms.\n",
    "\n",
    "Boyd and Ellison define social network sites as web-based services that allow individuals to\n",
    "* construct a public or semi-public profile within a bounded system,\n",
    "* articulate a list of other users with whom they share a connection, and\n",
    "* view and traverse their list of connections and those made by others within the system.\n",
    "This information highlights the structural aspects of social media, such as user profiles, connections, and network visibility.\n",
    "\n",
    "The Merriam-Webster's Dictionary definition gives emphasis to the communication and community building aspects of social media: the social networks are forms of electronic communication (such as websites for social networking and microblogging) through which users create online communities to share information, ideas, personal messages, and other content (such as videos) (\"Social media\" [*Merriam-Webster.com Dictionary*]).\n",
    "\n",
    "On the other hand, Cambridge Dictionary describes social media as technological platforms: websites and computer programs that allow people to communicate and share information on the internet using a computer or mobile phone (\"Social media\" [*Cambridge Dictionary*]).\n",
    "\n",
    "In short, we can sum up the aspects of social media as follows:\n",
    "* User profiles and connections\n",
    "* Online communities\n",
    "* Information and content sharing\n",
    "* Technological platforms\n",
    "* User-generated content\n",
    "* Interactivity and collaboration\n",
    "* Virtual networks\n",
    "* Expression and communication\n",
    "\n",
    "Each of the above characteristics is a data-generating process. For example, the \"User profiles and connections\" part of social media generates demographic data, the social network itself (friends, followers, and connections), and personal interests and preferences. The \"Online communities\" generate group membership data, community interaction patterns, and interests/trends within different communities.\n",
    "\n",
    "It should be clear by now that the vast amount of data generated can be used for multiple purposes that depend on the objectives of the researcher. Specifically, in the context of finance, we are very interested in the \"User-generated content\" from which we can extract:\n",
    "* text data: posts, comments\n",
    "* sentiment and opinion data\n",
    "* multimedia content related to financial topics\n",
    "\n",
    "This data is valuable for several finance-specific applications:\n",
    "\n",
    "* Market sentiment analysis: By analyzing the sentiment of social media posts about specific stocks, commodities, market conditions, etc., researchers can quantify public opinion.\n",
    "* Stock price prediction: Some studies have shown correlations between social media sentiment and short-term stock price changes, making this data potentially useful for trading strategies.\n",
    "* Financial news dissemination: Social media often spreads financial news faster than traditional media!\n",
    "* Risk assessment: By monitoring social media discussions, financial institutions can identify potential risks or reputational issues early.\n",
    "* Cryptocurrency trends: Given the significant role of online communities in cryptocurrency markets, social media data is particularly valuable in this sector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PGW_GpDyLIrs"
   },
   "source": [
    "## **2. Retrieving the Data**\n",
    "\n",
    "In this section, we will provide several examples of how to use an API in order to connect to a social media platform, how to retrieve the data, and how the data looks like before structuring it into dataframes for plotting and analysis.\n",
    "\n",
    "### **2.1 An Example of Quantifying Sentiment Using YouTube Data**\n",
    "\n",
    "In this subsection, we'll explore how to leverage YouTube API to gather and analyze public sentiment around significant financial events. We'll use the 2020 market crash as a case study to understand how sentiment can be quantified and correlated with financial data like the S&P 500 index.\n",
    "\n",
    "Pre-requisites:\n",
    "\n",
    "* Google Account: Ensure you have an active Google account. This is necessary to access Google's API services.\n",
    "\n",
    "* Google Cloud Project: You'll need to create a project in the Google Cloud Console. This project will host your API key, which is essential for accessing YouTube's data.\n",
    "\n",
    "YouTube API Setup: Follow these steps to get started:\n",
    "\n",
    "1. Go to the Google Cloud Console.\n",
    "\n",
    "2. Create a new project.\n",
    "\n",
    "3. Navigate to the API Library and enable the YouTube Data API v3.\n",
    "\n",
    "4. Create credentials (API key) for accessing the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ggHXUhfOLIrt",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import praw\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "import yfinance as yf\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SYbSayAdLIru"
   },
   "source": [
    "Key Considerations\n",
    "\n",
    "1. Quota Management: Be mindful of the API quota limits. Google provides a limited number of requests per day for free. Plan your data retrieval carefully to stay within these limits or you will be forced to stop your workflow for a day.\n",
    "\n",
    "2. Keyword Selection: The choice of keywords is critical. Think about the terms that were relevant during the time of the financial event. For our case study on the 2020 crash, we used terms like \"recession,\" \"economic crisis,\" \"inflation,\" etc.\n",
    "\n",
    "3. Channel Selection: Choose reputable and relevant channels that provide consistent and high-quality content.\n",
    "\n",
    "4. Data Analysis: After retrieving the data, you'll analyze the frequency of the keywords and correlate this with financial metrics like the S&P 500 index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 514,
     "status": "ok",
     "timestamp": 1730165350384,
     "user": {
      "displayName": "Greg Ciresi",
      "userId": "14255040454306907358"
     },
     "user_tz": 240
    },
    "id": "1uc9t_XtLIru",
    "outputId": "6638aee5-d96c-4451-e639-a767d2961dae",
    "tags": []
   },
   "outputs": [],
   "source": [
    "API_KEY = \"YOUR_API_KEY\"  \n",
    "CHANNEL_NAMES = ['cnbc'] # You can include your selection of Youtube finance channels\n",
    "CHANNEL_IDs = {} # Dictionary to store channel names and their IDs\n",
    "\n",
    "# Function to get channel ID by channel name\n",
    "def get_channel_id_by_name(channel_name, api_key):\n",
    "    url = f\"https://www.googleapis.com/youtube/v3/search?part=snippet&type=channel&q={channel_name}&key={api_key}\"\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    return data['items'][0]['snippet']['channelId']\n",
    "\n",
    "# Get channel IDs for all channel names\n",
    "for channel_name in CHANNEL_NAMES:\n",
    "    channel_id = get_channel_id_by_name(channel_name, API_KEY)\n",
    "    CHANNEL_IDs[channel_name] = channel_id\n",
    "\n",
    "# Print channel names and their IDs\n",
    "for channel_name, channel_id in CHANNEL_IDs.items():\n",
    "    print(f\"{channel_name}: {channel_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gdXaQiFNLIrw",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "KEYWORDS = ['recession', 'unemployment', 'crash', 'stimulus', 'crisis', 'inflation'] # List of keywords to search for. You can include your selection of relevant keywords\n",
    "START_DATE = '2020-01-01T00:00:00Z'\n",
    "END_DATE = '2020-05-30T23:59:59Z'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tHKtnIXiLIrx",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize YouTube API\n",
    "def initialize_youtube(api_key):\n",
    "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "    return youtube\n",
    "\n",
    "# Get videos from a channel\n",
    "def get_videos(youtube, channel_id, start_date, end_date):\n",
    "    request = youtube.search().list(\n",
    "        part='snippet',\n",
    "        channelId=channel_id,\n",
    "        publishedAfter=start_date,\n",
    "        publishedBefore=end_date,\n",
    "        maxResults=50,\n",
    "        type='video'\n",
    "    )\n",
    "    videos = []\n",
    "    while request:\n",
    "        response = request.execute()\n",
    "        videos.extend(response['items'])\n",
    "        request = youtube.search().list_next(request, response)\n",
    "    return videos\n",
    "\n",
    "# Get comments from a video\n",
    "def get_comments(youtube, video_id):\n",
    "    comments = []\n",
    "    request = youtube.commentThreads().list(\n",
    "        part='snippet',\n",
    "        videoId=video_id,\n",
    "        maxResults=100\n",
    "    )\n",
    "    while request:\n",
    "        try:\n",
    "            response = request.execute()\n",
    "            comments.extend(response['items'])\n",
    "            request = youtube.commentThreads().list_next(request, response)\n",
    "        except HttpError as e:\n",
    "            if e.resp.status == 403 and 'commentsDisabled' in str(e):\n",
    "                print(f\"Comments are disabled for video ID: {video_id}\")\n",
    "                break\n",
    "            else:\n",
    "                raise e\n",
    "    return comments\n",
    "\n",
    "# Filter data and count keyword mentions\n",
    "def filter_and_count_comments(comments, keywords, start_date, end_date):\n",
    "    keyword_counts = defaultdict(int)\n",
    "    start = datetime.strptime(start_date, '%Y-%m-%dT%H:%M:%SZ')\n",
    "    end = datetime.strptime(end_date, '%Y-%m-%dT%H:%M:%SZ')\n",
    "    current_date = start\n",
    "\n",
    "    while current_date <= end:\n",
    "        keyword_counts[current_date.strftime('%Y-%m-%d')] = 0\n",
    "        current_date += timedelta(days=1)\n",
    "\n",
    "    for comment_thread in comments:\n",
    "        comment = comment_thread['snippet']['topLevelComment']['snippet']\n",
    "        comment_date = datetime.strptime(comment['publishedAt'], '%Y-%m-%dT%H:%M:%SZ').strftime('%Y-%m-%d')\n",
    "        comment_text = comment['textDisplay'].lower()\n",
    "        if any(keyword in comment_text for keyword in keywords):\n",
    "            keyword_counts[comment_date] += 1\n",
    "\n",
    "    return keyword_counts\n",
    "\n",
    "def plot_keyword_counts(keyword_counts, channel_name):\n",
    "    dates = list(keyword_counts.keys())\n",
    "    counts = list(keyword_counts.values())\n",
    "    df = pd.DataFrame({'Date': dates, 'Count': counts})\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df = df.set_index('Date')\n",
    "    df.sort_index(inplace=True)\n",
    "\n",
    "    plt.figure(figsize=(14, 7))  # Adjust the figure size for better readability\n",
    "    ax = df.plot(kind='bar', legend=False, width = 0.8)\n",
    "    ax.set_title(f'Keyword Mentions per Day in {channel_name}')\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Count of Keywords')\n",
    "\n",
    "    # Set x-ticks at regular intervals and rotate labels for better readability\n",
    "    ax.set_xticks(range(0, len(df.index), 14))\n",
    "    ax.set_xticklabels(df.index.strftime('%Y-%m-%d')[::14], rotation=45, ha='right')\n",
    "\n",
    "    #plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qsW8I8ifLIrx",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Main script\n",
    "# THIS CELL CAN TAKE SIGNIFICANT TIME TO COMPLETE EVEN CLOSE TO 20 MINUTES.\n",
    "# PLEASE BE PATIENT.\n",
    "\n",
    "# Initialize the YouTube API client\n",
    "youtube = initialize_youtube(API_KEY)\n",
    "\n",
    "# Fetch videos from the specified channel within the date range\n",
    "videos = get_videos(youtube, channel_id, START_DATE, END_DATE)\n",
    "\n",
    "# Initialize a list to store all comments\n",
    "all_comments = []\n",
    "\n",
    "# Fetch comments for each video\n",
    "for video in videos:\n",
    "    video_id = video['id']['videoId']\n",
    "    try:\n",
    "        comments = get_comments(youtube, video_id)\n",
    "        all_comments.extend(comments)\n",
    "    except HttpError as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Filter and count keywords in comments\n",
    "keyword_counts = filter_and_count_comments(all_comments, KEYWORDS, START_DATE, END_DATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RiaS5GpFu7W0"
   },
   "source": [
    "In the cell below, you can see what a response looks like. You can use your imagination to think of how the data can be used. For example, the `likeCount` key can be used to assess whether a specific comment should be weighted more than the rest. The `totalReplyCount` values can be used to identify controversial, favorite, or interesting topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 195,
     "status": "ok",
     "timestamp": 1730165690911,
     "user": {
      "displayName": "Greg Ciresi",
      "userId": "14255040454306907358"
     },
     "user_tz": 240
    },
    "id": "VV10l0dqYv5q",
    "outputId": "2ca9e283-be53-4cdb-d372-e14bdbb2dc8b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "comments[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 528
    },
    "executionInfo": {
     "elapsed": 2011,
     "status": "ok",
     "timestamp": 1730165695232,
     "user": {
      "displayName": "Greg Ciresi",
      "userId": "14255040454306907358"
     },
     "user_tz": 240
    },
    "id": "hzFNpQ0kLIry",
    "outputId": "19aa1014-e53a-4175-cb23-603855c91ccd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preparing dataset for visualization\n",
    "df_keywords = pd.DataFrame(keyword_counts.items()).set_index(0)\n",
    "df_keywords.index = pd.to_datetime(df_keywords.index)\n",
    "df_keywords.sort_index(inplace=True)\n",
    "df_keywords.columns = ['count']\n",
    "df_keywords['count'] = df_keywords['count'].astype(int)\n",
    "df_keywords = df_keywords.resample('D').sum()\n",
    "\n",
    "rolling_sum = df_keywords.loc[:\"2020-05-30\"].rolling('7D').sum()\n",
    "\n",
    "# Downloading S&P500 as a benchmark\n",
    "gspc = yf.download('^GSPC', start='2020-01-01', end='2020-05-30')['Adj Close']\n",
    "\n",
    "# Plotting\n",
    "fig, ax1 = plt.subplots(figsize=(16, 7))\n",
    "\n",
    "# Plotting the keywords data\n",
    "#ax1.plot(df_keywords.loc[:\"2020-05-30\"].rolling('7D').sum(), color='green', alpha=0.6, label='Rolling weekly sum of keywords')\n",
    "ax1.bar(rolling_sum.index, rolling_sum.values.flatten(), color='green', alpha=0.6, label='Rolling weekly sum of keywords')\n",
    "#ax1.set_ylim(-1, 40)\n",
    "ax1.set_ylabel('Sum of Keywords')\n",
    "\n",
    "# Creating a twin axis to plot S&P500 data\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(gspc, color='blue', alpha=0.5, label='S&P500 Price')\n",
    "ax2.set_ylim(1500, 3400)\n",
    "ax2.set_ylabel('S&P500 Price')\n",
    "\n",
    "# Adding legends\n",
    "ax1.legend(loc='upper left')\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "plt.title('Rolling weekly sum of keywords and S&P500 Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DOc-O845LIrz"
   },
   "source": [
    "The graph above is an example of how one can quantify sentiment. The easiest way to do so is by counting the number of words in our choice, and given that some words are not easily found unless there is a reason, we can trust that the excess mentioning of such words indicates the sentiment at that time. The green bars represent the sum of keywords like \"recession,\" \"economic crisis,\" and \"inflation\" found in YouTube comments, indicating heightened public concern. The blue line tracks the S&P 500 index price, highlighting the market's reaction during the same period. As the keyword mentions continue to rise after March 2020, coinciding with the steep decline in the S&P 500, we observe the rising concern of the people and the impact of the crash on overall sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P429Kf7CLIrz"
   },
   "source": [
    "**Exercise 1**\n",
    "\n",
    "Choose an asset and a period when significant events for this asset occurred. Identify the keywords you want to count and the channel(s) that are appropriate for your analysis. Use the code provided and follow the same workflow as above. Plot the results (you may need to do serious modifications in the code) and paste it in the forum along with a \"what we see\" analysis.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FUOsMUatLIr0"
   },
   "source": [
    "### **2.2 Reddit**\n",
    "\n",
    "Reddit is a popular social news and discussion website where users can share content and engage in conversations on various topics, including cryptocurrency. For a financial engineering student, Reddit data can give valuable examples as to how to gain insight into market sentiment and trends.\n",
    "\n",
    "In this section, we'll explore how to use the Reddit API to collect and analyze data related to cryptocurrency discussions. The Reddit API allows developers to access Reddit's database of posts, comments, and other content programmatically. To use the API, you'll need to create a Reddit account and register an application to obtain the necessary credentials (client ID, client secret, and user agent). At this point, you are advised to use AI instructions in order to complete this step.\n",
    "\n",
    "Our example focuses on analyzing sentiment in cryptocurrency-related subreddits over the past month. This is due to the limitations of Reddit's API and the fact that it was not created for data analysis but merely as a tool for subreddit moderators. The code considers only the last month (the last month starting from the time you run the code cells), and for this reason, the results are expected to be different every time you run the code.\n",
    "\n",
    "You are STRONGLY advised to not spam the cell that downloads the comments as your app will be restricted from reaching these subreddits, even though that can be corrected by creating a new app. This is a good reminder to students that sentiment analysis data do not come for free. We are bounded by API restrictions and the absence of free historical data.\n",
    "\n",
    "It's also important to note that while we often expect sentiment analysis to align closely with price movements, this correlation isn't always perfect. Factors such as market manipulation, external news, or regulatory changes can cause divergences between sentiment and price. However, sentiment analysis remains a valuable tool for understanding market psychology and potential price directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 614,
     "status": "ok",
     "timestamp": 1730165699846,
     "user": {
      "displayName": "Greg Ciresi",
      "userId": "14255040454306907358"
     },
     "user_tz": 240
    },
    "id": "6jXo9n7KLIr0",
    "outputId": "82799f55-5217-4abe-b9ab-64a63a8a9bff",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the Reddit instance\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id='CLIENT_ID',\n",
    "    client_secret=\"CLIENT_SECRET\",\n",
    "    user_agent='USER_AGENT',\n",
    "    check_for_async=False\n",
    ")\n",
    "\n",
    "# Example of fetching the top posts from a subreddit\n",
    "submissions = reddit.subreddit('finance').top(limit=5)\n",
    "for submission in submissions:\n",
    "    print(f\"Title: {submission.title}\")\n",
    "    print(f\"Score: {submission.score}\")\n",
    "    print(f\"URL: {submission.url}\")\n",
    "    print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Pd_OIO4LIr1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define subreddits\n",
    "subreddits = ['cryptocurrency']\n",
    "\n",
    "# Define bull and bear sentiment keywords\n",
    "bull_keywords = ['bull', 'bullish', 'moon', 'lambo', 'hodl', 'buy', 'long', 'uptrend', 'breakout', 'rally']\n",
    "bear_keywords = ['bear', 'bearish', 'crash', 'dip', 'sell', 'short', 'downtrend', 'correction', 'dump', 'fud']\n",
    "\n",
    "# Function to get comments from a subreddit\n",
    "def get_comments(subreddit_name, start_date):\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    comments = []\n",
    "    for submission in subreddit.new(limit=None):\n",
    "        if submission.created_utc < start_date:\n",
    "            break\n",
    "        submission.comments.replace_more(limit=0)\n",
    "        for comment in submission.comments.list():\n",
    "            comments.append({\n",
    "                'text': comment.body,\n",
    "                'created_utc': datetime.fromtimestamp(comment.created_utc),\n",
    "                'subreddit': subreddit_name\n",
    "            })\n",
    "    return comments\n",
    "\n",
    "# Get comments from the last month\n",
    "end_date = datetime.now()\n",
    "start_date = end_date - timedelta(days=30)\n",
    "all_comments = []\n",
    "\n",
    "for subreddit in subreddits:\n",
    "    all_comments.extend(get_comments(subreddit, start_date.timestamp()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m47VRDBs99d7"
   },
   "source": [
    "NLTK (Natural Language Toolkit) is a leading platform for building Python programs to work with human language data. In our analysis, we use several NLTK components:\n",
    "\n",
    "* `VADER` (Valence Aware Dictionary and sEntiment Reasoner) lexicon: A rule-based sentiment analysis tool specifically attuned to sentiments expressed in social media.\n",
    "* `SentimentIntensityAnalyzer`: A VADER-based tool that provides sentiment scores for text.\n",
    "* `word_tokenize`: A function that splits text into individual words or tokens.\n",
    "\n",
    "These tools allow us to process and analyze the sentiment of Reddit comments effectively, providing insights into the overall mood of cryptocurrency discussions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4106,
     "status": "ok",
     "timestamp": 1730166062400,
     "user": {
      "displayName": "Greg Ciresi",
      "userId": "14255040454306907358"
     },
     "user_tz": 240
    },
    "id": "8mjTWuWq94ga",
    "outputId": "d03781bb-19f7-4f47-96fe-fe1cdaaf63eb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download the VADER lexicon\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Initialize the sentiment analyzer\n",
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GBHscfhPQox0"
   },
   "source": [
    "**Exercise 2**\n",
    "\n",
    "Explain the differences between the `.split(\" \")` function and `word_tokenize` by using the documentation. Display those differences by using a paragraph. Explain with simple words what tokenization is in the NLP context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w5ZCvhNAhyes",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preparing the dataset for visualization\n",
    "\n",
    "text_df = pd.DataFrame(all_comments) # Create a DataFrame from the comments\n",
    "\n",
    "text_df['tokenized'] = text_df['text'].apply(word_tokenize) # Tokenize the comments\n",
    "text_df['tokenized'] = text_df['tokenized'].apply(lambda x: [word.lower() for word in x]) # Convert words to lowercase\n",
    "3669\n",
    "text_df['bull_count'] = text_df['tokenized'].apply(lambda x: sum(1 for word in x if word in bull_keywords)) # Count bull keywords\n",
    "text_df['bear_count'] = text_df['tokenized'].apply(lambda x: sum(1 for word in x if word in bear_keywords)) # Count bear keywords\n",
    "\n",
    "text_df['sia'] = text_df['text'].apply(lambda x: sia.polarity_scores(x)['compound']) # Calculate sentiment using NLTK\n",
    "\n",
    "text_df.set_index('created_utc', inplace=True) # Set the index to the created_utc column\n",
    "text_df.sort_index(inplace=True) # Sort the DataFrame by the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1721747404475,
     "user": {
      "displayName": "Christos Koutkos",
      "userId": "05788829401179389824"
     },
     "user_tz": -180
    },
    "id": "WcjtaIOKhyhd",
    "outputId": "4a6401a9-f662-41fd-b3e8-3444539081fa",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Downloading BTCUSD price as a cryptomarket proxy.\n",
    "btcusd = yf.download('BTC-USD', start=start_date, end=end_date)['Adj Close'] # Download Bitcoin price data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 438
    },
    "executionInfo": {
     "elapsed": 1801,
     "status": "ok",
     "timestamp": 1721747406274,
     "user": {
      "displayName": "Christos Koutkos",
      "userId": "05788829401179389824"
     },
     "user_tz": -180
    },
    "id": "8GKlFR4UAGIK",
    "outputId": "a6feeaf3-fea2-4179-e3ad-9488b21ef13e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Superimposing the price on the smoothened sentiment score\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(14, 5)) # Create a figure and axis\n",
    "ax2 = ax1.twinx() # Create a second axis that shares the same x-axis\n",
    "\n",
    "smoothened_sia = text_df[['sia']].resample('D').mean().rolling('7D').mean() # Calculate the rolling mean of SIA\n",
    "ax1.plot(smoothened_sia, color='green', label='Sentiment Analysis Score') # Plot the SIA mean\n",
    "ax2.plot(btcusd, color='blue', label='BTC-USD') # Plot the Bitcoin price\n",
    "\n",
    "ax1.set_ylabel('SIA Sentiment') # Set the label for the first axis\n",
    "ax2.set_ylabel('BTC-USD Price') # Set the label for the second axis\n",
    "ax1.set_xlabel('Date') # Set the label for the x-axis\n",
    "\n",
    "# Get the handles and labels from both axes\n",
    "lines_1, labels_1 = ax1.get_legend_handles_labels()\n",
    "lines_2, labels_2 = ax2.get_legend_handles_labels()\n",
    "\n",
    "# Combine the handles and labels, and create a legend\n",
    "ax1.legend(lines_1 + lines_2, labels_1 + labels_2, loc='upper left')\n",
    "\n",
    "plt.title('SIA Sentiment vs. BTC-USD Price') # Set the title of the plot\n",
    "plt.show() # Show the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gpE3WoKdSvlW"
   },
   "source": [
    "Let's do the same as above, but now, let's choose our own keywords in estimating the sentiment. We will aggregate the count of each bullish word and each bearish word. In order to have a sentiment estimation, we will use the difference bull - bear. This difference will be smoothed by a 7-day window (as in the SIA case) before we superimpose the BTCUSD price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rDHVQ0KCLIr4",
    "tags": []
   },
   "outputs": [],
   "source": [
    "keywords_bull_bear = text_df[['bull_count', 'bear_count']].resample('D').sum()\n",
    "bull_minus_bear = keywords_bull_bear['bull_count'] - keywords_bull_bear['bear_count']\n",
    "bull_minus_bear = bull_minus_bear.rolling('7D').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 446
    },
    "executionInfo": {
     "elapsed": 930,
     "status": "ok",
     "timestamp": 1721747407197,
     "user": {
      "displayName": "Christos Koutkos",
      "userId": "05788829401179389824"
     },
     "user_tz": -180
    },
    "id": "MKVS8Xm1TN4P",
    "outputId": "ef6f44d2-ed62-4d8b-ff06-37358796736b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(14, 5)) # Create a figure and axis\n",
    "ax2 = ax1.twinx() # Create a second axis that shares the same x-axis\n",
    "\n",
    "\n",
    "ax1.plot(bull_minus_bear, color='green', label='Bull - Bear') # Plot the difference\n",
    "ax2.plot(btcusd, color='blue', label='BTC-USD') # Plot the Bitcoin price\n",
    "\n",
    "ax1.set_ylabel('Bull - Bear counts') # Set the label for the first axis\n",
    "ax2.set_ylabel('BTC-USD Price') # Set the label for the second axis\n",
    "ax1.set_xlabel('Date') # Set the label for the x-axis\n",
    "\n",
    "# Get the handles and labels from both axes\n",
    "lines_1, labels_1 = ax1.get_legend_handles_labels()\n",
    "lines_2, labels_2 = ax2.get_legend_handles_labels()\n",
    "\n",
    "# Combine the handles and labels, and create a legend\n",
    "ax1.legend(lines_1 + lines_2, labels_1 + labels_2, loc='upper left')\n",
    "\n",
    "plt.title('Bull - Bear counts vs. BTC-USD Price') # Set the title of the plot\n",
    "plt.show() # Show the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mldVH4s-TwcB"
   },
   "source": [
    "**Exercise 3**\n",
    "\n",
    "In the Reddit section, we have downloaded all the comments during a specific period for one subreddit. By using the code provided, you can select the keywords (given the subreddit we have already selected) that will help you search additional trends (specific altcoins, specific meme coins, etc). Use both the `nltk` and the manual way of estimating sentiment. Paste your results in the forums."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PRo2wFOGtwFg"
   },
   "source": [
    "## **3. Conclusion**\n",
    "\n",
    "In conclusion, this lesson has demonstrated the powerful role that social media data can play in financial analysis. We've explored how to retrieve and analyze data from major platforms like YouTube and Reddit, using API interactions and employing simple sentiment analysis techniques. These methods provide valuable insights into market sentiment, public opinion, and the trends that can impact financial markets.\n",
    "\n",
    "The examples given are significant in the sense of what one can achieve with a **free** API. Keep in mind that data for sentiment analysis are hard to find and most likely expensive. But by carefully choosing your period and channels, subreddits, tweets, etc., you will be able to extract, quantify, and visualize the sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rKKS0keOFrc1"
   },
   "source": [
    "**References**\n",
    "\n",
    "* Boyd, Danah M., and Nicole B. Ellison. \"Social Network Sites: Definition, History, and Scholarship.\" Journal of Computerâ€Mediated Communication, vol. 13, no. 1, 2007, pp. 210-230.\n",
    "\n",
    "* \"Social media.\" *Cambridge Dictionary*, Cambridge University Press. https://dictionary.cambridge.org. Accessed 18 Nov. 2024.\n",
    "\n",
    "* \"Social media.\" *Merriam-Webster.com Dictionary*, Merriam-Webster, https://www.merriam-webster.com. Accessed 18 Nov. 2024."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "KvTOJQ-GFrc1"
   },
   "source": [
    "---\n",
    "Copyright 2024 WorldQuant University. This\n",
    "content is licensed solely for personal use. Redistribution or\n",
    "publication of this material is strictly prohibited.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
