{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A_djuWRzMXkq"
   },
   "source": [
    "MODULE 4 | LESSON 2\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gsLi2o3XRQIy"
   },
   "source": [
    "# **Search Engine Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LfsSxOHrueYp"
   },
   "source": [
    "|  |  |\n",
    "|:---|:---|\n",
    "| **Reading Time**  |  60 minutes  |\n",
    "| **Prior Knowledge**  |  Basic Python, Basic concepts of Natural Language Processing, Financial Markets  |\n",
    "| **Keywords**  | Machine Learning, Google Trends, Pytrends, Sentiment Analysis, TF-IDF, LSA (Latent Semantic Analysis), NLP (Natural Language Processing) |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VJxHYrXBObSC"
   },
   "source": [
    "*In this lesson we explore how to leverage data science and machine learning for financial analysis, with a focus on analyzing search terms from Google Trends data. It covers techniques like TF-IDF and LSA to extract insights from text, and demonstrates how to gather and analyze Google Trends data using Python and the `pytrends` library. The notebook also discusses the application of these techniques for sentiment analysis, risk management, and portfolio optimization.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "executionInfo": {
     "elapsed": 8094,
     "status": "error",
     "timestamp": 1734919822882,
     "user": {
      "displayName": "Greg Ciresi",
      "userId": "14255040454306907358"
     },
     "user_tz": 300
    },
    "id": "jwm5p_SgJWVs",
    "outputId": "82eaa5b4-23fa-4a77-c1ef-aab7b267e8d7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import re\n",
    "import time\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from pytrends.request import TrendReq\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Download 'punkt' resource\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UPGYpr11b7VS"
   },
   "source": [
    "## **1 What is Search Engine Data**\n",
    "\n",
    "Search engine data refers to the information collected and generated by search engines as users interact with them. This data encompasses various aspects of user behavior, preferences, and interactions within the search engine ecosystem. Search engine data is a valuable alternative data source that can complement traditional financial data and provide a unique perspective on market trends and investor behavior.\n",
    "\n",
    "When it comes to financial engineering, using search engine data is a more specialized area. We generaly have two options: Using APIs from major search engines such as Google Trends API or Bing API; or Alternative data provides: Several companies specialize in providing alternative data for financial analysis, including search engine data.\n",
    "\n",
    "In this lesson we will focus on using Google Trends data. Here are some suggestions on how Google Trends data can be used for the financial engineering tasks:\n",
    "\n",
    " - **Sentiment analysis:** Google Trends data can be used to measure the sentiment of the market towards a particular security or asset. This can be done by tracking the search volume for keywords related to the security or asset. For example, if the search volume for \"Tesla stock price\" is increasing, this could be a sign that the market is becoming more bullish on Tesla.\n",
    " - **Predictive modeling:** Google Trends data can be used to build predictive models for financial markets. This can be done by using machine learning algorithms to train models on historical Google Trends data and other relevant data sources. For example, a model could be trained to predict the price of a stock based on the search volume for keywords related to the stock.\n",
    " - **Risk management:** Google Trends data can be used to identify and manage risks in financial markets. This can be done by tracking the search volume for keywords related to risk factors, such as \"economic recession\" or \"market volatility.\" For example, if the search volume for \"economic recession\" is increasing, this could be a sign that the market is becoming more concerned about the possibility of a recession.\n",
    " - **Algorithmic trading:** Google Trends data can be used to develop algorithmic trading strategies. This can be done by using Google Trends data to identify trading opportunities. For example, a trading strategy could be developed to buy a stock when the search volume for keywords related to the stock is increasing.\n",
    "\n",
    "\n",
    "Search engine data can be valuable for investors, and TF-IDF can play a role in its analysis. Here's how:\n",
    "\n",
    "**How Search Engine Data Helps Investors:**\n",
    "\n",
    " - Gauging Public Interest: Search engine data, like Google Trends, reveals the relative popularity of search terms over time. This can indicate public interest in specific companies, products, industries, or economic trends.\n",
    " - Identifying Emerging Trends: Increases in search volume for specific terms can signal emerging trends or shifts in consumer behavior, providing early investment opportunities.\n",
    " - Sentiment Analysis: Analyzing search queries related to a company or asset can provide insights into public sentiment, which can influence investment decisions.\n",
    " - Predictive Modeling: Search engine data can be combined with other data sources to build predictive models for market movements or company performance.\n",
    "\n",
    "**Role of TF-IDF in Analyzing Search Engine Data:**\n",
    "\n",
    " - Keyword Analysis: TF-IDF can be applied to analyze the search queries themselves, identifying the most relevant and informative terms related to an investment topic.\n",
    " - Document Analysis: TF-IDF can be used to analyze the content of web pages returned in search results, helping investors understand the context and sentiment surrounding a particular search term.\n",
    " - Trend Identification: Tracking changes in the TF-IDF scores of specific terms over time can help identify emerging trends or shifts in public interest.\n",
    "\n",
    " - Examples:\n",
    "\n",
    "   - An investor might analyze Google Trends data for search terms related to electric vehicles to assess the growing interest in this industry and identify potential investment opportunities.\n",
    "   - A hedge fund could use TF-IDF to analyze news articles and social media posts related to a company to gauge public sentiment and predict potential stock price movements.\n",
    "\n",
    "**Benefits for Investors:**\n",
    "\n",
    " - Early Signals: Search engine data can provide early signals of market trends or changes in investor sentiment.\n",
    " - Alternative Data Source: It offers an alternative data source that can complement traditional financial data.\n",
    " - Improved Decision-Making: By incorporating search engine data analysis, investors can make more informed investment decisions.\n",
    "\n",
    "**Considerations:**\n",
    "\n",
    " - Data Quality: Search engine data can be noisy and may not always accurately reflect market sentiment.\n",
    " - Privacy Concerns: Ethical considerations and data privacy regulations need to be taken into account when using search engine data.\n",
    "\n",
    "Overall, search engine data can be a valuable tool for investors, and TF-IDF can be effectively used to analyze this data and extract meaningful insights for investment decisions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p7hvvGNgtM9-"
   },
   "source": [
    "### **1.1 Introduction to Google Trends data**\n",
    "\n",
    "It is important to note that Google Trends data is just one of many data sources that can be used for financial engineering tasks. It is important to use a variety of data sources to get a complete picture of the market.\n",
    "\n",
    "In this lesson, we will explore how to use Python to fetch and analyze data from Google Trends, a powerful tool that reveals how often a particular search term is entered relative to the total search volume across various regions of the world and in various languages. We will use the `pytrends` library, a Python interface for Google Trends data. This library allows us to automate the downloading of reports from Google Trends. We will learn how to fetch the interest over time for a specific search term, how to analyze this data using Python, and how to visualize findings. It provides an interface to Google Trends, allowing to:\n",
    "\n",
    " - Fetch Interest Over Time: See how search interest for specific keywords has changed over a period.\n",
    " - Analyze Interest by Region: Discover where in the world people are searching for particular terms.\n",
    " - Find Related Queries: Identify top and rising search queries related to our keywords.\n",
    "\n",
    "By the end of this section, we will have a solid understanding of how to leverage Python and Google Trends data to uncover insights about search trends. This knowledge can be particularly useful in fields like market research, where understanding trends can provide valuable insights into consumer behavior.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L9zICmd7JsRJ"
   },
   "source": [
    "### **1.2 Fetching and Comparing Interest Over Time**\n",
    "\n",
    "Now let's define the search terms that we want to compare. In this case, we'll compare “AAPL”, “TSLA”, and “NVDA”. We fetch the interest over last 12 month period for these search terms from Google Trends and visualize the data. The following code snippet initializes an instance of the `TrendReq` class from the `pytrends` library and defines a list of keywords and a timeframe. This sets up the necessary objects to interact with Google Trends and retrieve data. Then the code builds the payload for the Google Trends request using the defined keywords and timeframe, and then retrieves the interest over time data. The `interest_over_time_df` DataFrame would then contain the Google Trends data for the specified keywords and timeframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "executionInfo": {
     "elapsed": 1207,
     "status": "ok",
     "timestamp": 1734919211812,
     "user": {
      "displayName": "Greg Ciresi",
      "userId": "14255040454306907358"
     },
     "user_tz": 300
    },
    "id": "dIKwwYmvXC1m",
    "outputId": "c02c298d-7856-4ad2-8f03-bfd08c384ea6"
   },
   "outputs": [
    {
     "ename": "TooManyRequestsError",
     "evalue": "The request failed: Google returned a response with code 429",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTooManyRequestsError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m timeframe \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoday 12-m\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m# for latest 12 months replace with 'today 12-m'\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Build the payload and get the interest over time data\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[43mpytrends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_payload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkw_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeframe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m interest_over_time_df \u001b[38;5;241m=\u001b[39m pytrends\u001b[38;5;241m.\u001b[39minterest_over_time()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Create a figure and axes and plot the data\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pytrends/request.py:189\u001b[0m, in \u001b[0;36mTrendReq.build_payload\u001b[0;34m(self, kw_list, cat, timeframe, geo, gprop)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_payload[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreq\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mdumps(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_payload[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreq\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# get tokens\u001b[39;00m\n\u001b[0;32m--> 189\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pytrends/request.py:195\u001b[0m, in \u001b[0;36mTrendReq._tokens\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Makes request to Google to get API tokens for interest over time, interest by region and related queries\"\"\"\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# make the request and parse the returned json\u001b[39;00m\n\u001b[0;32m--> 195\u001b[0m widget_dicts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTrendReq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGENERAL_URL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTrendReq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPOST_METHOD\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken_payload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrim_chars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwidgets\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# order of the json matters...\u001b[39;00m\n\u001b[1;32m    202\u001b[0m first_region_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pytrends/request.py:159\u001b[0m, in \u001b[0;36mTrendReq._get_data\u001b[0;34m(self, url, method, trim_chars, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m status_codes\u001b[38;5;241m.\u001b[39mcodes\u001b[38;5;241m.\u001b[39mtoo_many_requests:\n\u001b[0;32m--> 159\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTooManyRequestsError\u001b[38;5;241m.\u001b[39mfrom_response(response)\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mResponseError\u001b[38;5;241m.\u001b[39mfrom_response(response)\n",
      "\u001b[0;31mTooManyRequestsError\u001b[0m: The request failed: Google returned a response with code 429"
     ]
    }
   ],
   "source": [
    "# Initialize an instance of the TrendReq class with US region and 360 timezone\n",
    "pytrends = TrendReq(hl='en-US', tz=360)\n",
    "\n",
    "# Define the list of keywords and timeframe\n",
    "kw_list = [\"AAPL\", \"TSLA\", \"NVDA\"]\n",
    "timeframe = 'today 12-m' # for latest 12 months replace with 'today 12-m'\n",
    "\n",
    "# Build the payload and get the interest over time data\n",
    "pytrends.build_payload(kw_list, timeframe=timeframe)\n",
    "interest_over_time_df = pytrends.interest_over_time()\n",
    "\n",
    "# Create a figure and axes and plot the data\n",
    "plt.figure(figsize=(16, 6))\n",
    "for term in kw_list:\n",
    "  plt.plot(interest_over_time_df.index, interest_over_time_df[term], label=term)\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Trends Index')\n",
    "plt.title('Interest Over Time')\n",
    "plt.legend(loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ARPxlEN8lTFg"
   },
   "source": [
    "Now that we have the plot, here are some key things to pay attention to and analyze:\n",
    "\n",
    "**Overall Trends:**\n",
    " - Upward/Downward: Are the trends generally increasing, decreasing, or remaining stable over the year? This can indicate growing or waning interest in the companies.\n",
    " - Seasonality: Do we notice any recurring patterns or spikes at certain times of the year? This could be related to product releases, financial reports, or other events.\n",
    "\n",
    "**Relative Popularity:**\n",
    " - Comparison: Which keyword shows the highest overall interest? Which one shows the lowest? This can give us an idea of the relative popularity of these companies in terms of search volume.\n",
    " - Crossovers: Are there points where the trends for different keywords intersect? This could signal shifts in public attention or perception.\n",
    "\n",
    "**Volatility:**\n",
    " - Fluctuations: How much do the trends fluctuate over time? Large swings might suggest sensitivity to news or market events.\n",
    " - Outliers: Are there any significant spikes or dips that stand out? Try to investigate what might have caused them (e.g., news announcements, product launches, controversies).\n",
    "\n",
    "**Correlation:**\n",
    " - Relationship: Do the trends for different keywords seem to move together or independently? This can indicate whether they are influenced by similar factors or not.\n",
    "\n",
    "By carefully examining these aspects of the plot, we can gain insights into the search behavior related to these companies and potentially draw conclusions about their public perception and market performance.\n",
    "\n",
    "For our example specifically, peaks and troughs can be observed, with NVIDIA (NVDA) experiencing a significant peak around March 2024, nearly reaching a trend index of 80, and then again over most recent period by end of August 2024. Suc high index could coincide with significant events related to NVIDIA, such as product launches, earnings reports, or market developments. In fact, in March 2024, NVIDIA made significant announcements related to its next-generation GPU architecture called Blackwell. This architecture boasts an impressive 208 billion transistors and is designed to run real-time generative AI models. The anticipation around Blackwell likely contributed to the increased interest in NVIDIA during that time. Additionally, in August 2024, NVIDIA continued to unveil innovations related to AI and data center performance at the Hot Chips conference, further fueling interest in the company. These events and technological advancements likely explain the observed spikes in interest for NVIDIA during those months.\n",
    "\n",
    "Apple (AAPL) and Tesla (TSLA) exhibit varying trends over the same period. While both companies generally maintain interest, their fluctuations are less pronounced compared to NVIDIA. Analyzing news or corporate announcements during these timeframes might shed light on the reasons behind these trends.\n",
    "\n",
    "We also need to consider exploring correlations between the interest trends in these companies and broader market indices (e.g., S&P 500). If there's a strong positive correlation, it could indicate that overall market sentiment influences interest in specific stocks.\n",
    "\n",
    "We also need to observe any recurring patterns across different months or quarters. Seasonal factors (e.g., holiday seasons, earnings seasons) might impact investor sentiment and interest levels.\n",
    "\n",
    " - TSLA Peaks: Tesla (TSLA) usually experiences noticeable peaks around April and July each year. These could align with quarterly earnings reports or significant announcements related to Tesla's products or business developments.\n",
    " - AAPL Trends: Apple (AAPL) usually shows a consistent upward trend around September and January. These months often coincide with new product launches (e.g., iPhone releases) and holiday seasons.\n",
    " - NVDA Fluctuations: NVIDIA (NVDA) exhibits less predictable patterns, but there are fluctuations around March and August. Investigate whether industry-specific events or conferences occur during these months."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nZ4GALQwIwUK"
   },
   "source": [
    "### **1.3 Analyzing Interest by other metrics**\n",
    "\n",
    "Google Trends also allow to fetch other details in relation to our search query:\n",
    "\n",
    " - **Interest by subregion** lets us see how interest in a particular search term varies across different geographical areas within a larger region.\n",
    " - **Related topics** are broader subjects or concepts that are associated with our keyword, and which were also searched by other users.\n",
    " - **Related queries** are specific search terms that people have used in conjunction with our keyword. These are actual queries entered into the search engine by other users during the same time period.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L8keXe70ep85"
   },
   "source": [
    "#### **Analyzing Interest by Region**\n",
    "\n",
    "Now, let's fetch the interest by region for a search term NVDA and identify the top 10 regions with the highest interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "executionInfo": {
     "elapsed": 789,
     "status": "ok",
     "timestamp": 1734919220516,
     "user": {
      "displayName": "Greg Ciresi",
      "userId": "14255040454306907358"
     },
     "user_tz": 300
    },
    "id": "77fTNAKdbtDv",
    "outputId": "b1bce5ff-6858-4d08-a515-3642ad940ab4"
   },
   "outputs": [
    {
     "ename": "TooManyRequestsError",
     "evalue": "The request failed: Google returned a response with code 429",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTooManyRequestsError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize an instance of the TrendReq class and build the payload\u001b[39;00m\n\u001b[1;32m      2\u001b[0m pytrends \u001b[38;5;241m=\u001b[39m TrendReq(hl\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men-US\u001b[39m\u001b[38;5;124m'\u001b[39m, tz\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m360\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mpytrends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_payload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNVDA\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeframe\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m2023-09-01 2024-09-01\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Get the interest by region data\u001b[39;00m\n\u001b[1;32m      6\u001b[0m interest_by_region_df \u001b[38;5;241m=\u001b[39m pytrends\u001b[38;5;241m.\u001b[39minterest_by_region()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pytrends/request.py:189\u001b[0m, in \u001b[0;36mTrendReq.build_payload\u001b[0;34m(self, kw_list, cat, timeframe, geo, gprop)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_payload[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreq\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mdumps(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_payload[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreq\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# get tokens\u001b[39;00m\n\u001b[0;32m--> 189\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pytrends/request.py:195\u001b[0m, in \u001b[0;36mTrendReq._tokens\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Makes request to Google to get API tokens for interest over time, interest by region and related queries\"\"\"\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# make the request and parse the returned json\u001b[39;00m\n\u001b[0;32m--> 195\u001b[0m widget_dicts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTrendReq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGENERAL_URL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTrendReq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPOST_METHOD\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken_payload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrim_chars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwidgets\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# order of the json matters...\u001b[39;00m\n\u001b[1;32m    202\u001b[0m first_region_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pytrends/request.py:159\u001b[0m, in \u001b[0;36mTrendReq._get_data\u001b[0;34m(self, url, method, trim_chars, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m status_codes\u001b[38;5;241m.\u001b[39mcodes\u001b[38;5;241m.\u001b[39mtoo_many_requests:\n\u001b[0;32m--> 159\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTooManyRequestsError\u001b[38;5;241m.\u001b[39mfrom_response(response)\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mResponseError\u001b[38;5;241m.\u001b[39mfrom_response(response)\n",
      "\u001b[0;31mTooManyRequestsError\u001b[0m: The request failed: Google returned a response with code 429"
     ]
    }
   ],
   "source": [
    "# Initialize an instance of the TrendReq class and build the payload\n",
    "pytrends = TrendReq(hl='en-US', tz=360)\n",
    "pytrends.build_payload([\"NVDA\"], timeframe='2023-09-01 2024-09-01')\n",
    "\n",
    "# Get the interest by region data\n",
    "interest_by_region_df = pytrends.interest_by_region()\n",
    "interest_by_region_df.sort_values(by='NVDA', ascending=False).head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kpia0deVvJTe"
   },
   "source": [
    "This output shows the top 10 regions with the highest search interest for \"NVDA\" during the specified period ('2023-09-01 2024-09-01'). The values represent the relative popularity of the search term in each region, where 100 is the highest value.\n",
    "\n",
    "For example, Hong Kong has the highest search interest for \"NVDA\" with a value of 100, followed by Taiwan and Israel with a value of 79 each.\n",
    "\n",
    "This data can be useful for businesses to understand the geographic distribution of interest in their products or services. It can also be used for market research and to identify potential target markets. Please note that further analysis and correlation with other data sources would be needed to draw definitive conclusions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WLlY8KksJ8EK"
   },
   "source": [
    "#### **Related Queries and Topics**\n",
    "\n",
    "Top and rising related topics and queries can be extremely useful for financial engineering tasks because they provide insights into the collective \"mind of the market\" and reveal trends that might not be obvious from traditional financial data.\n",
    "\n",
    "**Related Topics** are broader themes associated with NVDA. This can help to identify:\n",
    " - Industry trends: Are there broader trends in gaming, AI, or data centers that are influencing interest in NVDA?\n",
    " - Investment themes: Are there specific investment themes (e.g., growth stocks, tech stocks) that are driving interest in NVDA?\n",
    "\n",
    "**Related Queries** are other terms people search for in conjunction with NVDA keyword. This can reveal:\n",
    " - Emerging trends: Are there new or rising keywords that indicate shifting interest or new areas of focus for NVDA?\n",
    " - Competitor analysis: Are people searching for NVDA in comparison to its competitors?\n",
    " - Public perception: What are the main concerns or interests people have regarding NVDA?\n",
    "\n",
    "Both Related Topics and Related Queries can be filtered to get Top and Rising terms for each:\n",
    "\n",
    " - **Top** - these are most popular search topics/queries. Scoring is on a relative scale from 100 as the most commonly searched topic/querie, 50 for query searched half as often, and so on.\n",
    " - **Rising** - these are topics/queries with the biggest increase in search frequency since the last time period. Results marked \"Breakout\" had a tremendous increase, probably because these topics/queries are new and had few (if any) prior searches.\n",
    "\n",
    "\n",
    "In this lesson we will focus on Related Queries. Both top and rising related queries offer valuable insights, but they serve different purposes. The best choice depends on specific goals and the type of analysis we are conducting. From a financial engineering perspective, the choice between top and rising related queries still depends on our specific goals, but here's a more tailored recommendation:\n",
    "\n",
    " - **For Algorithmic Trading and Sentiment Analysis:** Rising queries are generally more valuable. They can provide real-time signals for sentiment-based trading strategies. We could build a system that automatically adjusts portfolio positions or generates trading signals based on changes in the sentiment expressed in rising queries. For example if we observe a sudden surge in negative sentiment related to a company we hold in our portfolio, algorithm could automatically reduce exposure to that stock.\n",
    "\n",
    " - **For Risk Management:** Rising queries are crucial for early warning signs. A spike in searches for terms related to lawsuits, regulatory investigations, or product recalls could indicate potential risks for a company or industry. We can integrate rising query analysis into risk management system to identify and assess potential threats proactively.\n",
    "\n",
    " - **For Portfolio Optimization and Asset Allocation:** Both top and rising queries can be useful.\n",
    "   - Top queries: Can help to understand the main themes and drivers of different assets or industries, allowing to diversify portfolio accordingly.\n",
    "   - Rising queries: Can help to identify emerging themes or potential risks that might warrant adjustments to portfolio.\n",
    "\n",
    " - **For Quantitative Research and Modeling:** Both types of queries can be valuable as alternative data sources.\n",
    "    - Rising queries: Can be used to create sentiment indicators or event detection signals that can be incorporated into quantitative models.\n",
    "    - Top queries: Can be used to understand the relationships between different assets or industries based on the themes and concerns expressed in search queries.\n",
    "\n",
    "Overall, while both have their uses, rising queries tend to be more valuable for financial engineering applications that require real-time insights and a focus on short-term trends and sentiment shifts. Top queries provide a broader understanding of market perception, which can be useful for longer-term strategies and fundamental analysis. Remember to consider the challenges of noise, data quality, and the need for sophisticated analysis techniques when working with both types of queries.\n",
    "\n",
    "Please also note that narrowing the time period for fetching insights from rising related queries can be very beneficial, especially when our goal is to detect short-term trends or capture immediate market reactions. Shorter timeframe generally provides a good balance between sensitivity, data availability, and actionability. We can experiment with different timeframes to find what works best for our specific needs and the volatility of the assets we are analyzing.\n",
    "\n",
    "Let's now fetch the top and rising related queries for 'NVDA' search term for the period from 2024-05-01 to 2024-09-01 and analyze the results. This time we manually downloaded the data in CSV format and saved locally as 'NVDA-relatedQueries.csv'. CSV file from Google Trends containing related queries typically presents the data in a stacked format:  The \"Rising\" queries data is typically stacked below the \"Top\" queries data in the same CSV file. There might be a blank row or a visual separator (like a line of dashes) to distinguish the two sections. The code below addresses the specific structure of the Google Trends CSV file, particularly the stacking of \"Top\" and \"Rising\" queries. First we read CV file into DataFrame. Then find the start and end rows for each section and finally extract each section with simultaneously splitting column on comma and formatting values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "executionInfo": {
     "elapsed": 10252,
     "status": "error",
     "timestamp": 1734919309978,
     "user": {
      "displayName": "Greg Ciresi",
      "userId": "14255040454306907358"
     },
     "user_tz": 300
    },
    "id": "fEdLAD2XdZfM",
    "outputId": "3bbf36b9-115c-4481-cf6d-7dab86817b9c"
   },
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "df = pd.read_csv('NVDA-relatedQueries.csv', header=None, delimiter='\\t')\n",
    "\n",
    "# Find the start and end rows for each section\n",
    "top_start = df[df.iloc[:, 0] == 'TOP'].index[0] + 1\n",
    "top_end = df[df.iloc[:, 0] == 'RISING'].index[0] - 1\n",
    "rising_start = df[df.iloc[:, 0] == 'RISING'].index[0] + 1\n",
    "rising_end = df.shape[0]  # End of the DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>top_query</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>stock nvda</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nvda price</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>stock price nvda</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tsla</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>amd</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>amd stock</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>aapl</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>tsla stock</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>aapl stock</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>amzn</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>msft</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>nvda earnings</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>smci</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>amzn stock</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>msft stock</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>tesla stock</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>meta</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>smci stock</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>meta stock</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>nvda split</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>gme</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>qqq</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>spy</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>pltr</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           top_query value\n",
       "3         stock nvda   100\n",
       "4         nvda price    23\n",
       "5   stock price nvda    20\n",
       "6               tsla     7\n",
       "7                amd     7\n",
       "8          amd stock     7\n",
       "9               aapl     7\n",
       "10        tsla stock     6\n",
       "11        aapl stock     6\n",
       "12              amzn     5\n",
       "13              msft     5\n",
       "14     nvda earnings     5\n",
       "15              smci     5\n",
       "16        amzn stock     4\n",
       "17        msft stock     4\n",
       "18       tesla stock     4\n",
       "19              meta     4\n",
       "20        smci stock     4\n",
       "21        meta stock     4\n",
       "22        nvda split     3\n",
       "23               gme     3\n",
       "24               qqq     3\n",
       "25               spy     3\n",
       "26              pltr     3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract top queries\n",
    "queries_top = df.iloc[top_start:top_end].copy()\n",
    "queries_top = queries_top.rename(columns={queries_top.columns[0]: 'top_queries'})\n",
    "\n",
    "# Split the 'top_queries' column and convert 'value' to numeric\n",
    "queries_top[['top_query', 'value']] = queries_top['top_queries'].str.split(',', expand=True)\n",
    "queries_top = queries_top[['top_query', 'value']]  \n",
    "queries_top.loc[:, 'value'] = pd.to_numeric(queries_top['value'])\n",
    "queries_top\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rising_query</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>ffie stock</td>\n",
       "      <td>3200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>ffie</td>\n",
       "      <td>3150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>serv stock</td>\n",
       "      <td>2800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>asts</td>\n",
       "      <td>750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>asts stock</td>\n",
       "      <td>700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>nvda split date</td>\n",
       "      <td>700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>nvda stock split date</td>\n",
       "      <td>550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>gme stock</td>\n",
       "      <td>550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>gme</td>\n",
       "      <td>550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>gamestop stock</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>hims stock</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>cmg stock</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>cmg stock price</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>dell stock price</td>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>dell stock</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>chipotle stock</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>broadcom stock</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>djt stock price</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>cava stock</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>djt stock</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>nvda split</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>avgo stock</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>crowdstrike stock</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>nvax stock</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>nke stock</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             rising_query value\n",
       "29             ffie stock  3200\n",
       "30                   ffie  3150\n",
       "31             serv stock  2800\n",
       "32                   asts   750\n",
       "33             asts stock   700\n",
       "34        nvda split date   700\n",
       "35  nvda stock split date   550\n",
       "36              gme stock   550\n",
       "37                    gme   550\n",
       "38         gamestop stock   400\n",
       "39             hims stock   250\n",
       "40              cmg stock   250\n",
       "41        cmg stock price   200\n",
       "42       dell stock price   190\n",
       "43             dell stock   150\n",
       "44         chipotle stock   150\n",
       "45         broadcom stock   140\n",
       "46        djt stock price   130\n",
       "47             cava stock   110\n",
       "48              djt stock   110\n",
       "49             nvda split   100\n",
       "50             avgo stock   100\n",
       "51      crowdstrike stock   100\n",
       "52             nvax stock   100\n",
       "53              nke stock    90"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract rising queries\n",
    "queries_rising = df.iloc[rising_start:rising_end].copy()\n",
    "queries_rising = queries_rising.rename(columns={queries_rising.columns[0]: 'rising_queries'})\n",
    "\n",
    "# Split the 'rising_queries' column aon the first comma\n",
    "queries_rising[['rising_query', 'value']] = queries_rising['rising_queries'].str.split(',', n=1, expand=True)\n",
    "queries_rising = queries_rising[['rising_query', 'value']]\n",
    "\n",
    "# Clean the 'value' column\n",
    "queries_rising['value'] = queries_rising['value'].str.replace(r'[^0-9.-]', '', regex=True)  # Keep only numbers, dots, and minus signs\n",
    "queries_rising.loc[:, 'value'] = pd.to_numeric(queries_rising['value'], errors='coerce')  # Convert to numeric\n",
    "\n",
    "# Display 'rising_queries'\n",
    "queries_rising"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3mzrPZiopKdz"
   },
   "source": [
    "The rising queries for \"NVDA\" provides some interesting insights, particularly regarding the upcoming earnings announcement. Here's what we can gather:\n",
    "\n",
    " - Earnings Dominate: The overwhelming focus is on NVDA's earnings. Terms related to earnings are all experiencing massive increases in search interest. This signifies a high level of anticipation and interest in the company's financial results.\n",
    "\n",
    " - Real-Time Information: The \"Breakout\" trend for \"nvda earnings time today\" suggests that people are actively seeking real-time information about the earnings announcement, possibly during the day of the release.\n",
    "\n",
    " - Global Interest: The presence of Japanese and Chinese characters indicates that interest in NVDA's earnings extends beyond English-speaking regions, highlighting the company's global reach.\n",
    "\n",
    " - Investor Focus: Terms like \"nvda investor relations\" and \"when does nvda report earnings\" suggest that investors are actively seeking information about the earnings release and possibly preparing for potential market reactions.\n",
    "\n",
    " - Financials and Nasdaq: The rise of \"nasdaq nvda financials\" suggests that people are looking for detailed financial information about NVDA, possibly on the Nasdaq website or other financial platforms.\n",
    "\n",
    " - Other Stocks and Potential Noise: While earnings-related terms dominate, there are some outliers like \"lunr stock,\" \"pdd stock,\" \"crm stock,\" \"chwy stock,\" and \"afrm stock\" These might be related to other companies or market trends and could potentially be noise in the context of analyzing NVDA. Further investigation would be needed to understand their relevance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kRswql41LkrO"
   },
   "source": [
    "### **1.4 Considerations and Limitations when using Google Trends data**\n",
    "\n",
    "When using Google Trends data for financial analysis, we should be aware of its limitations. While Google Trends data can be insightful, it's crucial to understand its limitations regarding data quality. Here are some key aspects to keep in mind:\n",
    "\n",
    " - **Sampling:** Google Trends data is based on a sample of searches, not the entire search volume and the exact sampling methodology isn't publicly disclosed. This means there's inherent sampling error, and the data might not perfectly represent the entire population of searches. This means there might be some noise and the data may not perfectly reflect the actual search interest.\n",
    " - **Data Updates:** Google Trends data might be updated or revised over time as Google refines its algorithms and data processing techniques. This means that historical data you accessed previously could potentially change slightly in the future.\n",
    " - **Normalization:** Google Trends data is normalized, meaning the values are relative to the highest point in the timeframe you select. The data is normalized to a scale of 0 to 100. A value of 100 represents the peak popularity, while other values are scaled proportionally. This means the values represent the relative popularity of a search term compared to the highest point in the selected timeframe. While normalization helps in comparing trends, it can also mask the absolute search volume, which might be relevant in some cases.\n",
    " - **Geographic Granularity:** The level of geographic detail available in Google Trends data varies depending on the search term and the time period. For some terms or regions, the data might be more granular, while for others, it might be more aggregated. This can affect the accuracy of regional analysis.\n",
    " - **External Factors:** Search behavior can be influenced by various external factors, such as media coverage, news events, seasonality, and even the popularity of other search terms. These factors can introduce noise and bias into the data, making it challenging to isolate the true underlying trends.\n",
    " - **Privacy:** Google Trends data is anonymized and aggregated, but it's still important to be mindful of privacy considerations, especially when dealing with sensitive topics.\n",
    "\n",
    "\n",
    "By understanding these data quality considerations and taking appropriate precautions, we can use Google Trends data more effectively and draw more reliable conclusions for financial analysis. Rememebr that context is key. We need to consider the context surrounding the data, such as potential external factors that might be influencing search behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "soZKEBuf5nL5"
   },
   "source": [
    "## **2 Latent Semantic Analysis**\n",
    "\n",
    "Up until now we explored Google Trends data and explored simple examples of using `pytrends` to gather data. While `pytrends` itself provides valuable insights into search trends, incorporating machine learning techniques like Latent Semantic Analysis (LSA) can offer several advantages.\n",
    "\n",
    "Before intrducing LSA, we need to understand some terminology. In the context of LSA, a \"**document**\" refers to a unit of text that we want to analyze. It can be anything from a single sentence to a whole article, a book, or even a collection of related texts. The key is that it represents a distinct piece of information that we want to compare and analyze in relation to other documents. Here are some examples of what could be considered a \"document\" in different LSA applications:\n",
    "\n",
    " - **Topic Modeling:** Each document could be a news article, a blog post, or a social media post. LSA can help identify the main topics discussed across these documents.\n",
    " - **Information Retrieval:** Each document could be a web page, a research paper, or a product description. LSA can help find documents that are semantically similar to a given query.\n",
    " - **Document Classification:** Each document could be an email, a customer review, or a legal contract. LSA can help categorize these documents based on their content.\n",
    "\n",
    "Latent Semantic Analysis (LSA) is a technique in natural language processing that analyzes relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms. LSA assumes that words that are close in meaning will occur in similar pieces of text. Incorporating machine learning techniques like LSA can offer several advantages:\n",
    "\n",
    " - **Uncovering Hidden Relationships:** LSA can reveal hidden relationships between terms and concepts that might not be apparent from simply looking at raw search data. This can help us understand the underlying semantic structure of the search queries related to our documents.\n",
    "\n",
    " - **Dimensionality Reduction:** LSA can reduce the dimensionality of our data by representing it in a lower-dimensional concept space. This can make it easier to analyze and visualize the data, especially when dealing with a large number of terms.\n",
    "\n",
    " - **Improved Search and Recommendation:** By understanding the relationships between terms and concepts, we can build more intelligent search engines and recommendation systems. For example, we could suggest related searches or recommend documents based on semantic similarity rather than just keyword matching.\n",
    "\n",
    " - **Topic Modeling:** LSA can be used for topic modeling, which allows us to discover the main topics or themes present in a set of documents or search queries. This can help us understand the broader context of the information and potentially categorize or cluster documents more effectively.\n",
    "\n",
    "In a nutshell, LSA is used to analyze textual data and is a method for extracting and representing the contextual-usage meaning of words by statistical computations applied to a large corpus of text. LSA is a powerful tool for analyzing textual data, but it has some limitations. For example, it is not very good at handling polysemy (words with multiple meanings) or synonymy (different words with the same meaning).\n",
    "\n",
    "In this lesson we will explore basic example of how we could use LSA to analyze related search terms and improve our understanding of user search behavior. The choice of what constitutes a document depends on our specific application and the kind of analysis we want to perform. It's important to ensure that our documents are meaningful and relevant to the task at hand. A collection of documents is collectively called as \"**term-document matrix**\".\n",
    "\n",
    "Financial engineers might approach document composition for LSA or similar techniques in different ways:\n",
    "\n",
    " - Domain Expertise: Financial engineers often have deep domain expertise and carefully select keywords and phrases based on their understanding of financial markets, specific assets, and relevant events.\n",
    "\n",
    " - Data Sources: They may use a wider range of data sources beyond just Google Trends, such as News articles and sentiment analysis, Social media data, Company filings and earnings call transcripts, Proprietary datasets.\n",
    "\n",
    " - Sophisticated Preprocessing: They might employ more advanced natural language processing techniques for preprocessing, including Named entity recognition (identifying companies, people, locations), Sentiment analysis (assigning sentiment scores to text), Topic modeling (identifying underlying topics in documents).\n",
    "\n",
    " - Ensemble Methods: They may combine LSA with other machine learning techniques or use ensemble methods to improve the accuracy and robustness of their models.\n",
    "\n",
    " - Backtesting and Validation: Financial engineers rigorously backtest and validate their models using historical data to ensure they are reliable and can potentially generate profitable trading signals or investment strategies.\n",
    "\n",
    "Fow our example we will create term-document matrix with the following set of documents:\n",
    "\n",
    " > Document 1: \"Risk management in derivatives trading\", \\\n",
    " > Document 2: \"Portfolio optimization techniques and strategies\", \\\n",
    " > Document 3: \"Algorithmic trading and its impact on market efficiency\", \\\n",
    " > Document 4: \"Quantitative methods for financial modeling\", \\\n",
    " > Document 5: \"High-frequency trading and liquidity provision\", \\\n",
    " > Document 6: \"Integrating Derivatives into Portfolio Optimization Strategies\", \\\n",
    " > Document 7: \"High-Frequency Trading Strategies: Algorithmic Approaches and Market Impact\", \\\n",
    " > Document 8: \"Quantitative Methods for Algorithmic Trading and Portfolio Optimization\" \\\n",
    "\n",
    "Before proceeding with code, let's observe that documents in our term-document matrix contain such words as \"in\", \"and\", \"for\", \"on\". These words are called **stop words**. They are common words that generally don't carry much specific meaning in the context of text analysis. It's often beneficial to remove stop words from text data before creating the term-document matrix. This can help to:\n",
    "\n",
    " - **Reduce noise:** Stop words can add noise to our data and obscure the more important terms that convey the main topics or themes.\n",
    " - **Improve efficiency:** Removing stop words can reduce the size of our matrix, making computations faster and more efficient.\n",
    " - **Focus on relevant terms:** By eliminating common words, we can focus on the more meaningful terms that are specific to our documents and application.\n",
    "\n",
    "We will also apply lemmatization technique. **Stemming and lemmatisation** are two text preprocessing techniques used in Natural Language Processing (NLP) that help to reduce words to their base form.\n",
    " - **Stemming** reduces words to their stem, which are not necessarily real words, by removing prefixes and sufixes. For example \"playing\" would become \"play\", \"studies\" would become \"studi\", and \"happily\" would become \"happi\".\n",
    " - **Lemmatization** reduce words to their dictionary form or base form known as lemma. For example \"playing\" would become \"play\", \"studies\" would become \"study\", and \"better\" would become \"good\".\n",
    "\n",
    "In our example we consider lemmatization: Given the specific and potentially nuanced language of finance, lemmatization is likely to be more beneficial than stemming. It can help group different forms of words (e.g., \"analyze,\" \"analyzing,\" \"analysis\") while preserving their core meaning.\n",
    "\n",
    "Now that we made clear of all basic termonology and LSA definition, let's proceed with preparing our data. The following Python code performs exactly that:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 825
    },
    "executionInfo": {
     "elapsed": 268,
     "status": "error",
     "timestamp": 1734919327806,
     "user": {
      "displayName": "Greg Ciresi",
      "userId": "14255040454306907358"
     },
     "user_tz": 300
    },
    "id": "kueEqQ32A_2Q",
    "outputId": "6f52a736-03c0-4888-847e-747b72b8d22d"
   },
   "outputs": [],
   "source": [
    "# Sample documents\n",
    "documents = [\n",
    "    \"Risk management in derivatives trading\",\n",
    "    \"Portfolio optimization techniques and strategies\",\n",
    "    \"Algorithmic trading and its impact on market efficiency\",\n",
    "    \"Quantitative methods for financial modeling\",\n",
    "    \"High-frequency trading and liquidity provision\",\n",
    "    \"Integrating Derivatives into Portfolio Optimization Strategies\",\n",
    "    \"High-Frequency Trading Strategies: Algorithmic Approaches and Market Impact\",\n",
    "    \"Quantitative Methods for Algorithmic Trading and Portfolio Optimization\"\n",
    "]\n",
    "\n",
    "# Download stopwords and WordNet resources and then create a Lemmatizer Object\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# construct text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[^\\w\\s-]', '', text)  # Remove punctuation except hyphens\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens] # Lemmatize\n",
    "    stop_words = set(stopwords.words('english')) # define stop_words as a set of english stop words\n",
    "    tokens = [token for token in tokens if token not in stop_words] # Remove stop words\n",
    "    return tokens\n",
    "\n",
    "# Create a CountVectorizer object\n",
    "vectorizer = CountVectorizer(tokenizer=preprocess_text, token_pattern=None)\n",
    "\n",
    "# Create the term-document matrix, then convert to a pandas DataFrame\n",
    "term_document_matrix = vectorizer.fit_transform(documents)\n",
    "df = pd.DataFrame(term_document_matrix.toarray(), columns = vectorizer.get_feature_names_out())\n",
    "term_document_matrix = df.T\n",
    "print(term_document_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gTC8Jl08FjBK"
   },
   "source": [
    "This matrix shows the relationships between the terms (rows) and documents (columns). For example:\n",
    "\n",
    " - The term \"algorithmic\" appears (value of 1) in documents 3, 7 and 8.\n",
    " - The term \"trading\" appears in documents 1, 3, 5, 7 and 8.\n",
    " - Document 1 contains the terms \"derivatives\", \"management\", \"risk\" and \"trading\".\n",
    "\n",
    "This matrix is the foundation for applying LSA or other dimensionality reduction techniques.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iBQ8N9wm94Z0"
   },
   "source": [
    "### **2.1 How LSA relate to SVD**\n",
    "\n",
    "LSA uses Singular Value Decomposition (SVD) at its core to find the hidden relationships between terms and concepts. Singular Value Decomposition is a fundamental concept in linear algebra and a powerful technique used in many areas, including machine learning and natural language processing. Here's how it works:\n",
    "\n",
    " 1. Document-Term Matrix: LSA starts with a document-term matrix, where each row represents a document and each column represents a term (word). The cells contain the frequency of each term in each document.\n",
    "\n",
    " 2. SVD Application or Matrix Factorization: SVD is applied to Document-Term matrix. SVD is a way to factorize (break down) a matrix into three separate matrices. It decomposes the matrix into three matrices:\n",
    "\n",
    "   - $U$: Represents document-to-concept similarity.\n",
    "   - $\\Sigma$: A diagonal matrix containing singular values, representing the importance of each concept.\n",
    "   - $V$: Represents term-to-concept similarity.\n",
    "\n",
    "   - The Formula: The relationship between the original matrix ($A$) and the decomposed matrices is: $A = U \\Sigma V^T$ (where $V^T$ is the transpose of $V$).\n",
    "\n",
    " 3. Dimensionality Reduction: By keeping only the largest singular values (and corresponding columns/rows in $U$ and $V$), we reduce the number of dimensions (concepts) while preserving the most important information. This is similar to Principal Component Analysis (PCA).\n",
    "\n",
    " 4. Concept Space: The resulting matrices represent documents and terms in a new \"concept space\". Words with similar meanings will be closer together in this space, even if they don't appear in the same documents.\n",
    "\n",
    "In essence, SVD in LSA helps to:\n",
    "\n",
    " - **Uncover latent relationships:** Identify underlying concepts that are not explicitly present in the text.\n",
    " - **Reduce noise:** Filter out less important information and focus on the core meaning.\n",
    " - **Improve information retrieval:** Represent documents and queries in a way that captures semantic similarity.\n",
    "\n",
    "This process allows LSA to go beyond simple keyword matching and understand the meaning of text at a deeper level.\n",
    "\n",
    "Let's now proceed with python code where we will use `TruncatedSVD` from `scikit-learn`, essentially performing Singular Value Decomposition (SVD) under the hood. `TruncatedSVD` is an efficient implementation of SVD specifically designed for dimensionality reduction. It calculates the SVD and then keeps only the top `n_components` singular values and their corresponding vectors. This truncated version of the SVD is what produces the `lsa_matrix`, which represents our data in a lower-dimensional concept space.\n",
    "\n",
    "**Important:** The output of LSA can be different each time we run the code. This is due to the random initialization of the `TruncatedSVD` algorithm. The variation is related to the nature of Singular Value Decomposition (SVD) and how LSA uses it. Here's a breakdown of the key factors:\n",
    "\n",
    " - Rotation in Concept Space: SVD essentially finds the best way to represent our data in a new \"concept space.\" The components (or dimensions) in this space are determined by finding the directions of greatest variance in the original data. However, there is often some flexibility in how these components are oriented (rotated). Think of it like rotating a 3D object -- the object remains the same, but its projection onto the axes changes.\n",
    "\n",
    " - Sign Ambiguity: The signs (positive or negative) of the values in the LSA output can flip without changing the underlying meaning. This is because the direction of a component is arbitrary. Imagine a component representing \"financial modeling\" -- it doesn't matter if it points in one direction or the opposite direction; it still captures the same concept.\n",
    "\n",
    " - Sensitivity to Small Changes: SVD can be sensitive to small changes in the input data. Even minor variations in word frequencies or the addition/removal of a few documents can lead to some degree of rotation in the concept space, resulting in different term loadings.\n",
    "\n",
    " - Random Initialization: As mentioned before, the TruncatedSVD algorithm often uses random initialization, which can contribute to variations in the output.\n",
    "\n",
    "While these variations might seem concerning, they usually don't drastically change the overall interpretation of the LSA results. The relative positions of terms in the concept space and the general patterns of similarity are generally preserved. To ensure consistent results, we will set the `random_state` parameter to a fixed value when creating the `TruncatedSVD` object. This will produce the same output each time we run the code.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 252,
     "status": "ok",
     "timestamp": 1727368024780,
     "user": {
      "displayName": "Tea Tor",
      "userId": "14354052244333384547"
     },
     "user_tz": -60
    },
    "id": "hyL0Gb9xGTPJ",
    "outputId": "ab9d595c-fc8d-4a0c-a545-15b0c4779b27"
   },
   "outputs": [],
   "source": [
    "# Get the terms (words) from the term-document matrix\n",
    "terms = term_document_matrix.index\n",
    "\n",
    "# Create an LSA model and fit to term_document_matrix\n",
    "lsa = TruncatedSVD(n_components=2, random_state=42)\n",
    "lsa_matrix = lsa.fit_transform(term_document_matrix)\n",
    "\n",
    "# Create DataFrame for LSA results with term names and print it\n",
    "df_lsa = pd.DataFrame(lsa_matrix, index=terms)\n",
    "print(df_lsa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vUFlFxZZ7sgY"
   },
   "source": [
    "Interpreting the results of LSA involves understanding the new semantic space created by the dimensionality reduction and analyzing the relationships between terms and documents in this space. Here is how to interpret the results:\n",
    " - Interpretting the Columns: These are also known as Components or Concepts or Dimensions. Think of each component as representing a hidden \"concept\" or \"topic\" within documents.\n",
    " - Interpreting the rows: Each row in this matrix represents a term from our original matrix, but now projected onto a 2-dimensional concept space (because we chose `n_components=2`).\n",
    " - Interpreting the values:\n",
    "   - The two values in each row represent the coordinates of that term in the new concept space. Terms with similar coordinates are closer together in the concept space, indicating a semantic relationship. In our example first row represents the term \"algorithmic\". Its coordinates are approximately (1.488 -0.458). If another term, has similar coordinates, it suggests that these terms are related in the context of our documents.\n",
    "   - The values represent the loading of that term on the corresponding component. The loadings tell us how much each term contributes to that concept. For example, \"algorithmic\" and \"trading\" have high loadings on Component 1, suggesting that Component 1 represents the concept of \"algorithmic trading\". A higher value (positive or negative) indicates a stronger association between the term and that component.\n",
    "\n",
    "From the output of our code we can interpret the relationships between terms by looking at their coordinates and draw some preliminary observations:\n",
    "\n",
    " - **Distinct Clusters:**\n",
    "   - Trading Cluster: \"algorithmic\", \"high-frequency\", \"impact\", \"market\", and \"trading\" have high values for component 0 and mostly negative values for component 1. This reinforces the idea that component 0 represents concepts related to automated and high-frequency trading, and their potential impact on the market.\n",
    "\n",
    "   - Portfolio and Optimization Cluster: \"optimization\" and \"portfolio\" form a clear cluster with high positive values for both components. This suggests a strong association between these terms, likely related to portfolio optimization techniques.\n",
    "\n",
    "   - Derivatives and Method-Quantitative Cluster: Grouping \"method\", \"quantitative\", and \"derivative\" together suggests a potential connection between quantitative methods and their application to derivatives.\n",
    "\n",
    "   - Management-Risk-Liquidity-Provision Cluster: Combining \"management\", \"risk\", \"liquidity\", and \"provision\" suggests a cluster related to managing financial risk and ensuring liquidity. This also seems plausible, as these concepts are often intertwined in financial contexts, particularly when dealing with derivatives or trading activities.\n",
    "\n",
    " - **Other Observations:**\n",
    "   - \"Trading\" as a central concept: \"trading\" has the highest value for component 0, indicating its importance and potential connection to various forms of trading discussed in the documents.\n",
    "   - Distinct Pairs: The pair of terms \"financial\" and \"modeling\" form very distinct pair due to their identical coordinates. Further analysis is needed to determine their relationship to other terms and clusters.\n",
    "   - Overlap between clusters: Some terms, like \"strategy\" and \"technique\", have moderate values in multiple components, suggesting potential overlap or connections between different clusters.\n",
    "\n",
    "This quick analysis gives somewhat good overview of the relationships between terms based on the LSA results.\n",
    "\n",
    "We can now plot the coordinates to visualise them and confirm if these proposed clusters appear close together in visual representation. To achieve this, we will use `plotly.graph_objects` module imported as `go`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 704,
     "status": "ok",
     "timestamp": 1727368033290,
     "user": {
      "displayName": "Tea Tor",
      "userId": "14354052244333384547"
     },
     "user_tz": -60
    },
    "id": "TEX3QlGb4LH8",
    "outputId": "98def3d7-dfe3-4ca7-a091-8ac80e166926"
   },
   "outputs": [],
   "source": [
    "# Create a list of hover texts with labels and coordinates\n",
    "hover_texts = []\n",
    "for row in df_lsa.values:\n",
    "    terms = df_lsa.index[(df_lsa[0] == row[0]) & (df_lsa[1] == row[1])]\n",
    "    terms_str = ', '.join([f'\"{term}\"' for term in terms])\n",
    "    coord = f\"({row[0]:.5f}, {row[1]:.5f})\"\n",
    "    hover_texts.append(f\"Terms: {terms_str}<br>{coord}\")\n",
    "\n",
    "# Create the scatter plot\n",
    "fig = go.Figure(go.Scatter(\n",
    "    x=df_lsa[0],\n",
    "    y=df_lsa[1],\n",
    "    mode='markers',\n",
    "    marker=go.scatter.Marker(\n",
    "        size=10,\n",
    "        color='blue',\n",
    "        opacity=0.8\n",
    "    ),\n",
    "    hovertemplate='%{text}<extra></extra>',\n",
    "    text=hover_texts\n",
    "))\n",
    "\n",
    "# Set axis labels and title\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Component 0\",\n",
    "    yaxis_title=\"Component 1\",\n",
    "    title=\"LSA Results Scatter Plot\"\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNBfFZxMW9j4"
   },
   "source": [
    "Plotly plots are interactive by default. We can use its interactive features to pan/zoom and hover/select to see tooltips with term lables and coordinates.\n",
    "\n",
    "Now looking at this plot it seems that clusters are not that easy to visualise. It seems that we were too focused on the moderate values of term coordinates, without paying enough attention to the precise distances between them. To determine clusters more accurately, we should use a more formal methods. By using these methods, we can avoid subjective interpretations and define clusters more precisely based on quantitative measures of similarity or distance. We will consider these methods later in this lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y-nV2K1xTDQj"
   },
   "source": [
    "### **2.2 Important considerations with LSA and SVD**\n",
    "\n",
    "#### **Choosing the optimal number of components for LSA**\n",
    "\n",
    "Choosing the optimal number of components (`n_components`) for LSA is a crucial step. Here are some considerations:\n",
    "\n",
    " - **Start small:** Begin with a small number of components (e.g., 2, 3, or 5) to see if it gives a good initial understanding of the relationships between terms and documents. This can help to get a feel for the data and identify potential clusters or topics.\n",
    " - **Iteratively increase:** Gradually increase the number of components and observe how the interpretation of the results changes. Look for a point where adding more components doesn't significantly improve the interpretability or reveal new meaningful relationships.\n",
    " - **Consider the size of corpus:** For smaller corpora, fewer components might suffice to avoid overfitting. For larger corpora, more components could capture more subtle semantic relationships.\n",
    " - **Focus on semantic coherence:** Evaluate the semantic coherence of the resulting topics or clusters. Do the terms within each cluster make sense together? Are the clusters distinct and meaningful? If the coherence decreases as we add more components, it might be a sign that we are starting to overfit.\n",
    " - **Balance dimensionality reduction with information loss:** LSA involves a trade-off between reducing dimensionality and preserving information. We should try to find a sweet spot where we have a manageable number of components while still capturing the most important semantic relationships.\n",
    "\n",
    "In summary, there is no one-size-fits-all answer for the optimal number of components in LSA. It depends on specific data and goals. We would need to experiment with different values, carefully evaluate the results, and use domain knowledge to guide our choice.\n",
    "\n",
    "\n",
    "#### **Understanding limitations of LSA and SVD**\n",
    "\n",
    "As we observe from above example some terms are clustered as they appear in particular document. For example \"algorithmic\", \"high-frequency\", \"impact\", \"market\", and \"trading\" are similar terms making one cluster. And they are all from document 7.\n",
    "\n",
    "This is crucial point about Latent Semantic Analysis (LSA) and its potential limitations. If LSA consistently clusters terms solely based on their co-occurrence within the same document, it raises questions about its effectiveness in capturing broader semantic relationships.\n",
    "\n",
    "Here's a breakdown of why this might happen and what it means for LSA:\n",
    "\n",
    " - **Document-Specific Context:** LSA primarily relies on word co-occurrence patterns within a corpus. If terms frequently appear together in a single document but not across the entire corpus, LSA might interpret them as semantically related even if their broader contextual meanings differ significantly.\n",
    " - **Limited Corpus Size or Diversity:** A small or homogenous corpus can exacerbate this issue. If documents within the corpus lack diversity in topics and language use, LSA might struggle to identify broader semantic relationships beyond document-level co-occurrences.\n",
    " - **Data Sparsity:** In LSA, the term-document matrix can be very sparse, especially with a large vocabulary. This sparsity can sometimes lead to the model focusing on local co-occurrence patterns within documents rather than global semantic relationships.\n",
    "\n",
    "So, what's the point of LSA if it primarily identifies document-level clusters? While the scenario we encounter isn't ideal, LSA can still offer valuable insights:\n",
    "\n",
    " - **Document Summarization:** LSA can be effective in identifying key terms and concepts within individual documents, which is useful for summarization and topic extraction.\n",
    " - **Dimensionality Reduction:** Even if clusters are document-specific, LSA effectively reduces the dimensionality of the data, making it easier to analyze and visualize.\n",
    " - **Basis for Other Techniques:** LSA can serve as a foundation for more advanced topic modeling techniques that address its limitations, such as Latent Dirichlet Allocation (LDA).\n",
    "\n",
    "To improve LSA's performance and capture broader semantic relationships:\n",
    "\n",
    " - **Increase Corpus Size and Diversity:** Include a wider range of documents on different topics with diverse language use.\n",
    " - **Preprocessing and Feature Engineering:** Carefully preprocess the text (e.g., removing stop words, stemming/lemmatization) and consider using different weighting schemes (e.g., Term Frequency-Inverse Document Frequency or TF-IDF in short, a numerical statistic used in information retrieval and text mining to reflect how important a word is to a document in a collection or corpus.) to emphasize more informative terms.\n",
    " - **Explore Other Techniques:** Consider using alternative topic modeling techniques like LDA, which explicitly models the distribution of topics within documents.\n",
    "\n",
    "In summary, while LSA can be useful for document-level analysis, its limitations in capturing broader semantic relationships should be considered. Ensuring a diverse and representative corpus and exploring alternative techniques can help overcome these limitations and unlock the full potential of topic modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jDY-7WqkZTQv"
   },
   "source": [
    "## **3 Compute Similarity Measures**\n",
    "\n",
    "In this subsection we will appply several of the similarity measures to our LSA results to further analyse terms in LSA space.\n",
    "\n",
    "It is generally not best practice to rely solely on one similarity measure, especially when exploring complex datasets or tasks. Different similarity measures have different strengths and weaknesses and may capture different aspects of similarity.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HstJ9EsBeaqj"
   },
   "source": [
    "### **3.1 Cosine similarity measure between terms**\n",
    "\n",
    "Cosine similarity is a common way to measure the similarity between vectors in the LSA space. The following code helps to quantify the semantic similarity between terms within the first distinct cluster (terms: \"algorithmic\", \"high-frequency\", \"impact\", \"market\", \"trading\") based on their representations in the LSA concept space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 254,
     "status": "ok",
     "timestamp": 1727368041730,
     "user": {
      "displayName": "Tea Tor",
      "userId": "14354052244333384547"
     },
     "user_tz": -60
    },
    "id": "kZYsrinFONHe",
    "outputId": "73c4dc43-aed2-4238-e48d-7dfc1fb17fc8"
   },
   "outputs": [],
   "source": [
    "# Extract term vectors for first distinct cluster\n",
    "terms_cl1 = [\"algorithmic\", \"high-frequency\", \"impact\", \"market\", \"trading\"]\n",
    "term_vectors_cl1 = df_lsa.loc[terms_cl1]\n",
    "\n",
    "# Calculate cosine similarity\n",
    "similarity_matrix = cosine_similarity(term_vectors_cl1)\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "similarity_df = pd.DataFrame(similarity_matrix, index=terms_cl1, columns=terms_cl1)\n",
    "\n",
    "# Print the results for the first distinct cluster with labels\n",
    "print(\"Cosine similarity for first distinct cluster:\")\n",
    "print(similarity_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gT_-YYZqoufW"
   },
   "source": [
    "This matrix shows the pairwise cosine similarity between the terms in the first distinct cluster. The values range from 0 to 1, where 1 indicates perfect similarity and 0 indicates no similarity. Here are some observations:\n",
    "\n",
    " - High similarity within the cluster: Most of the values in the matrix are close to 1, indicating that the terms in this cluster are highly semantically similar. This is expected since they were identified as a cluster in the LSA analysis.\n",
    " - \"Impact\" and \"market\" are perfectly similar: The cosine similarity between \"impact\" and \"market\" is 1, suggesting that they are considered synonymous in the context of this corpus.\n",
    " - \"Algorithmic\" and \"trading\" are very similar: The cosine similarity between \"algorithmic\" and \"trading\" is very high (0.998), indicating a strong semantic connection between these terms. This makes sense as \"algorithmic trading\" is a common concept.\n",
    " - \"High-frequency\" is slightly less similar: \"High-frequency\" has slightly lower similarity scores compared to other terms, suggesting that it might have a slightly different meaning or context within this cluster.\n",
    "\n",
    "This analysis confirms that the terms in the first distinct cluster are indeed semantically related based on their cosine similarity.\n",
    "\n",
    "Let's now compute cosine similarity for terms in the fourth cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 246,
     "status": "ok",
     "timestamp": 1727368047474,
     "user": {
      "displayName": "Tea Tor",
      "userId": "14354052244333384547"
     },
     "user_tz": -60
    },
    "id": "fQi5r-genQkY",
    "outputId": "fc00f764-8799-424d-9dda-cfe5be73c2bd"
   },
   "outputs": [],
   "source": [
    "# Extract term vectors for fourth cluster\n",
    "terms_cl4 = [\"management\", \"risk\", \"liquidity\", \"provision\"]\n",
    "term_vectors_cl4 = df_lsa.loc[terms_cl4]\n",
    "\n",
    "# Calculate cosine similarity\n",
    "similarity_matrix = cosine_similarity(term_vectors_cl4)\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "similarity_df = pd.DataFrame(similarity_matrix, index=terms_cl4, columns=terms_cl4)\n",
    "\n",
    "# Print the results for the fourth cluster with labels\n",
    "print(\"Cosine similarity for fourth cluster:\")\n",
    "print(similarity_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vx13weMeXuzC"
   },
   "source": [
    "Here we have the following:\n",
    "\n",
    " - Perfect similarity: \"Management\" and \"risk\" have a cosine similarity of 1, indicating they are treated as highly related or even synonymous within this context. Similarly, \"liquidity\" and \"provision\" also show perfect similarity. This is not surprising because cosine similarity measures the angle between two vectors. Since \"Management\" and \"risk\" have identical coordinates, they point in the same direction, resulting in 0-degree angle and cosine similarity of 1. Similarly, \"liquidity\" and \"provision\" also form another pair of identical vectors.\n",
    "\n",
    " - Strong but not perfect similarity: While the other terms exhibit strong similarity, it's not perfect (cosine similarity of 0.82). This suggests a degree of semantic overlap but also some distinction in their meanings within the documents.\n",
    "\n",
    "This cluster clearly indicates a focus on managing risk, potentially highlighting the importance of liquidity provision in mitigating financial risks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O5PvXxbqtbko"
   },
   "source": [
    "We can continue this approach of selecting specific clusters and analyzing cosine similarity within those groups. This allows us to focus on terms that are already known to be related and uncover finer-grained semantic connections.\n",
    "\n",
    "By using a more targeted approach, we can gain more meaningful insights from cosine similarity analysis and better understand the semantic structure of our data.\n",
    "\n",
    "One way to expore relationship between clusters is to calculate the centroid of each cluster and then measure the cosine similarity between the centroids. This can help us understand how different topics or concepts relate to each other.\n",
    "\n",
    "Earlier we identified four distinct clusters and one pair Distinct Pair: The pair of terms \"financial\" and \"modeling\". Let's further analyse this pair to determine their relationship to other terms and clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 349,
     "status": "ok",
     "timestamp": 1727368053682,
     "user": {
      "displayName": "Tea Tor",
      "userId": "14354052244333384547"
     },
     "user_tz": -60
    },
    "id": "dGHzii16OND4",
    "outputId": "ca77c10c-219c-47a7-c224-24546ed19be1"
   },
   "outputs": [],
   "source": [
    "# Extract term vectors for distinct pair\n",
    "terms_pair = [\"financial\", \"modeling\"]\n",
    "term_vectors_pair = df_lsa.loc[terms_pair]\n",
    "\n",
    "# Extract term vectors for second and third cluster\n",
    "term_vectors_cl2 = df_lsa.loc[[\"optimization\", \"portfolio\"]]\n",
    "term_vectors_cl3 = df_lsa.loc[[\"derivative\", \"method\", \"quantitative\"]]\n",
    "\n",
    "\n",
    "# Calculate centroid for the distinct pair\n",
    "centroid_pair = term_vectors_pair.mean()\n",
    "\n",
    "# Store cluster term vectors in a list and calculate centroids for each cluster\n",
    "cluster_term_vectors = [term_vectors_cl1, term_vectors_cl2, term_vectors_cl3, term_vectors_cl4]\n",
    "centroids = [term_vectors.mean() for term_vectors in cluster_term_vectors]\n",
    "\n",
    "\n",
    "# Calculate cosine similarity between the distinct pair and each cluster centroid\n",
    "similarities = [cosine_similarity(centroid_pair.values.reshape(1, -1), centroid.values.reshape(1, -1)) for centroid in centroids]\n",
    "\n",
    "# Print the similarities\n",
    "for i, similarity in enumerate(similarities):\n",
    "  print(f\"Cosine similarity between distinct pair and cluster {i+1}:\", similarity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SEj-DT8E0nVJ"
   },
   "source": [
    "These results show the cosine similarity between the distinct pair and each of the four clusters. Here's how to interpret them:\n",
    "\n",
    " - Strongest similarity with cluster 2: The distinct pair has the highest cosine similarity (0.997) with cluster 2. This suggests that the pair is very closely related to the concepts or terms represented by cluster 2.\n",
    " - Moderate similarity with cluster 3: There's a moderate similarity (0.961) with cluster 3, indicating some overlap in meaning or context.\n",
    " - Weak similarity with clusters 1 and 4: The pair shows very weak similarity with clusters 1 (0.084) and 4 (0.056), suggesting that these clusters represent distinct concepts that are not closely related to the pair.\n",
    "\n",
    "Conclusion: Based on these results, we can conclude that the distinct pair is most strongly associated with cluster 2, with some moderate connection to cluster 3. It's likely that the pair shares a significant semantic overlap with the terms in cluster 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KeZnmrIw1bir"
   },
   "source": [
    "### **3.2 Cosine similarity between documents**\n",
    "\n",
    "Measuring cosine similarity between entire documents is one of the common applications of cosine similarity in natural language processing and information retrieval.\n",
    "\n",
    "To do this we first need to get **Document vectors**. There are few techniques to do this such as geting LSA vectors, Word embeddings and Term Frequency-Inverse Document Frequency (TF-IDF).\n",
    "\n",
    "In our lesson we will focus on TF-IDF vectors as these capture the importance of words in a document relative to a collection of documents. The following code calculates the cosine similarity between the documents using TF-IDF and presents the output in a dataframe.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "executionInfo": {
     "elapsed": 271,
     "status": "ok",
     "timestamp": 1727368061217,
     "user": {
      "displayName": "Tea Tor",
      "userId": "14354052244333384547"
     },
     "user_tz": -60
    },
    "id": "Yi6ggmtYXGuJ",
    "outputId": "0c2518d5-6cac-44d8-9bb5-28857c07a456"
   },
   "outputs": [],
   "source": [
    "# Create a TfidfVectorizer object and fit to to documents\n",
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=preprocess_text, token_pattern=None)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "# alculate the cosine similarity matrix\n",
    "cosine_sim = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "# Present TF-IDF matrix in DataFrame form\n",
    "short_labels = ['Doc ' + str(i+1) for i in range(len(documents))]\n",
    "df = pd.DataFrame(cosine_sim, index=short_labels, columns=short_labels)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tIreTb2Ki2L0"
   },
   "source": [
    "Here `TfidfVectorizer` is initialized with the same `preprocess_text` function we used previously for preprocessing document texts during LSA analysis. This function is passed as an argument to the tokenizer parameter inside `TfidfVectorizer`. By using the `preprocess_text` function as the tokenizer, we ensure that the TF-IDF calculation is based on the lemmatized and cleaned tokens, which can lead to more meaningful and accurate results. The `TfidfVectorizer` will apply this function to each document in the corpus to convert the raw text into a sequence of tokens (words or terms).\n",
    "\n",
    "Both `TfidfVectorizer` and `CountVectorizer` that we used earlier are used in `scikit-learn` for text analysis to convert a collection of text documents into a matrix of numerical features. However, they differ in how they represent the importance of words in the documents:\n",
    "\n",
    " - `CountVectorizer`: Creates a matrix where each entry represents the number of times a word appears in a document (term frequency). It's a simple way to represent text data numerically, but it doesn't consider the relative importance of words across the entire corpus. `CountVectorizer` focuses on the local importance of words within a document.\n",
    "\n",
    " - `TfidfVectorizer`: Creates a matrix where each entry represents the TF-IDF score of a word in a document. TF-IDF (Term Frequency-Inverse Document Frequency) takes into account both the frequency of a word in a document and its rarity across the entire corpus. This means that words that are frequent in a document but rare in the corpus will have higher TF-IDF scores, indicating their importance for that specific document. `TfidfVectorizer` focuses on the global importance of words relative to the entire corpus.\n",
    "\n",
    "Let's now analyse the code output. The above DataFrame shows the cosine similarity between the documents. Here are some observations based on the cosine similarity scores:\n",
    "\n",
    " - Document 6 and Document 2 are the most similar (0.54). This makes sense as both documents discuss portfolio optimization.\n",
    " - Document 7 and Document 3 are also quite similar (0.59), which is expected as they both cover algorithmic trading.\n",
    " - Document 8 seems to be moderately similar to several documents, including Document 2 (0.36), Document 6 (0.32), and Document 4 (0.42). This is because it covers a broader range of topics, including algorithmic trading and portfolio optimization.\n",
    " - Document 1 and Document 5 have a low similarity (0.10) with most other documents as they discuss risk management and liquidity provision, respectively, which are relatively broad topics.\n",
    "\n",
    "Here are some potential insights we could draw:\n",
    "\n",
    " - Documents related to portfolio optimization (2 and 6) form a cluster of related topics.\n",
    " - Algorithmic trading (3 and 7) is another cluster of related concepts.\n",
    " - Document 8 acts as a bridge between these two clusters, indicating some overlap between the topics.\n",
    " - Risk management (1) and liquidity provision (5) are relatively independent concepts within this dataset.\n",
    "\n",
    "Please note that these are just initial observations. One can delve deeper into the documents themselves to understand the nuances of their similarity and how the specific terms contribute to the cosine similarity scores.\n",
    "\n",
    "Remember that cosine similarity is just one measure of similarity. Depending on specific needs and the nature of data, we might need to explore other similarity measures as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2OVUcFcDrP_M"
   },
   "source": [
    "### **3.3 Jaccard index**\n",
    "\n",
    "Let's now consider using Jaccard index as measure to estimate similarity between our documents. In the following code snippet we constructed `jaccard_similarity` function that calculates the Jaccard similarity between two documents which are pre-processed. Inside this function we apply `intersection` and `union` built-in methods available for set objects in Python. And then Jaccard index is calculated by simply dividing size of the intersection by the size of union: `intersection / union`. We will apply this function to all possible pairs in our document corpus to compute all pairwise indexes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "executionInfo": {
     "elapsed": 262,
     "status": "ok",
     "timestamp": 1727368070074,
     "user": {
      "displayName": "Tea Tor",
      "userId": "14354052244333384547"
     },
     "user_tz": -60
    },
    "id": "ycwCpqtzfDH2",
    "outputId": "68ee22fc-c9c0-4a25-e418-8809884601ed"
   },
   "outputs": [],
   "source": [
    "# Sample documents\n",
    "documents = [\n",
    "    \"Risk management in derivatives trading\",\n",
    "    \"Portfolio optimization techniques and strategies\",\n",
    "    \"Algorithmic trading and its impact on market efficiency\",\n",
    "    \"Quantitative methods for financial modeling\",\n",
    "    \"High-frequency trading and liquidity provision\",\n",
    "    \"Integrating Derivatives into Portfolio Optimization Strategies\",\n",
    "    \"High-Frequency Trading Strategies: Algorithmic Approaches and Market Impact\",\n",
    "    \"Quantitative Methods for Algorithmic Trading and Portfolio Optimization\"\n",
    "]\n",
    "\n",
    "# Download stopwords, WordNet resources and create a Lemmatizer Object\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[^\\w\\s-]', '', text)  # Remove punctuation except hyphens\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens] # Lemmatize\n",
    "    stop_words = set(stopwords.words('english')) # define stop_words as a set of english stop words\n",
    "    tokens = [token for token in tokens if token not in stop_words] # Remove stop words\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# Function to calculate Jaccard similarity between two documents\n",
    "def jaccard_similarity(doc1, doc2):\n",
    "    # Preprocess documents\n",
    "    tokens1 = set(preprocess_text(doc1))\n",
    "    tokens2 = set(preprocess_text(doc2))\n",
    "\n",
    "    # Calculate Jaccard similarity\n",
    "    intersection = len(tokens1.intersection(tokens2))\n",
    "    union = len(tokens1.union(tokens2))\n",
    "    return intersection / union\n",
    "\n",
    "# Calculate Jaccard similarity matrix\n",
    "jaccard_sim = [[jaccard_similarity(doc1, doc2) for doc2 in documents] for doc1 in documents]\n",
    "\n",
    "# Present Jaccard similarity matrix in DataFrame form\n",
    "short_labels = ['Doc ' + str(i+1) for i in range(len(documents))]\n",
    "df = pd.DataFrame(jaccard_sim, index=short_labels, columns=short_labels)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DtDIE3oUtFlp"
   },
   "source": [
    "Here we observe:\n",
    " - Strong similarity between Documents 2 and 6 with highest similarity score (0.50) and also between documents 3 and 7 with also similarity score (0.50).\n",
    " - Moderate similarity: Document 8 again shows moderate similarity with Documents 2 (0.25), 4 (0.25) and 6 (0.22).\n",
    " - Low similarity: Documents 1 and 5 again exhibit low similarity.\n",
    "\n",
    "Comparison with Cosine similarity: The general patterns of similarity are relatively consistent with the cosine similarity results. However, Jaccard index tends to produce lower similarity scores overall, as it focuses on the overlap of terms and does not consider term frequency or inverse document frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GgWCFn-CtiYS"
   },
   "source": [
    "## **Conclusion**\n",
    "\n",
    "By the end of this lesson, you should have a solid understanding of how to use Python and Google Trends to analyze search trends. You should be able to fetch and compare the interest over time for multiple search terms, analyze interest by region, discover top and rising related queries. In this lesson we also explored how to leverage data science and machine learning for financial analysis, with a focus on analyzing search trends from Google Trends data. It covered techniques like TF-IDF and LSA to extract insights from text, and demonstrated how to gather and analyze Google Trends data using Python and the `pytrends` library. The notebook also discussed the application of these techniques for sentiment analysis, risk management, and portfolio optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QhepTOs6tiYT"
   },
   "source": [
    "**References**\n",
    "\n",
    " - Vrba, G. (2018). Timing The Market With Google Trends Search Volume Data. [online] Seeking Alpha. Available at: https://seekingalpha.com/article/4202781-timing-market-google-trends-search-volume-data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "KvTOJQ-GFrc1"
   },
   "source": [
    "---\n",
    "Copyright 2024 WorldQuant University. This\n",
    "content is licensed solely for personal use. Redistribution or\n",
    "publication of this material is strictly prohibited.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
